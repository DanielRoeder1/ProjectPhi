{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.modeling_phi import PhiForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code = True)\n",
    "config.know_type = \"kformer\"\n",
    "config.enc_dim = 768\n",
    "config.know_layer = [5,8,11,14,17,20,23]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1_5\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Write a detailed analogy between mathematics and a lighthouse.\n",
    "\n",
    "Answer:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\")\n",
    "out = model.generate(**inputs, max_length = 200)\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "df = datasets.load_from_disk(\"msmacro_wellformed_split\")\n",
    "df_pd = df[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, AutoConfig \n",
    "\n",
    "config = AutoConfig.from_pretrained(\"gpt2-medium\")\n",
    "config.know_type = \"gated_cross\"\n",
    "config.know_layer = [5,8,11,14,17,20,23]\n",
    "config.know_proj_bias = False\n",
    "config.hidden_dropout = 0.1\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from train_utils.eval import EvalCollator\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "dec_tokenizer.pad_token = dec_tokenizer.eos_token\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "dataset = load_dataset(\"squad\", split = \"validation\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})    \n",
    "loader = DataLoader(dataset, batch_size = 4, collate_fn = EvalCollator(dec_tokenizer,\n",
    "                                                                                enc_tokenizer, \n",
    "                                                                                mode = \"q\", \n",
    "                                                                                context_enc = True, \n",
    "                                                                                cover_labels=False, \n",
    "                                                                                context_column = \"qc\",\n",
    "                                                                                answer_column = \"answers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch, gen_batch, answers = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['h.5.crossattention.attention.proj_k.weight', 'h.23.crossattention.attention.proj_o.weight', 'h.11.crossattention.attention.proj_o.weight', 'h.5.ln_cross_attn.weight', 'h.14.crossattention.attention.proj_v.weight', 'h.14.crossattention.attn_gate', 'h.20.crossattention.attention.proj_q.weight', 'h.11.crossattention.attention.proj_v.weight', 'h.17.crossattention.attention.proj_v.weight', 'h.11.crossattention.attn_gate', 'h.14.crossattention.attention.proj_q.weight', 'h.17.crossattention.attention.proj_q.weight', 'h.8.crossattention.attention.proj_k.weight', 'h.23.ln_cross_attn.weight', 'h.23.crossattention.attention.proj_k.weight', 'h.8.crossattention.attention.proj_v.weight', 'h.5.crossattention.attention.proj_v.weight', 'h.23.crossattention.attention.proj_q.weight', 'h.14.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias', 'h.20.crossattention.attention.proj_k.weight', 'h.11.crossattention.attention.proj_k.weight', 'h.17.ln_cross_attn.weight', 'h.5.crossattention.attn_gate', 'h.20.ln_cross_attn.weight', 'h.14.crossattention.attention.proj_o.weight', 'h.11.ln_cross_attn.weight', 'h.17.crossattention.attn_gate', 'h.20.crossattention.attn_gate', 'h.17.crossattention.attention.proj_k.weight', 'h.5.crossattention.attention.proj_q.weight', 'h.8.crossattention.attention.proj_q.weight', 'h.17.ln_cross_attn.bias', 'h.5.crossattention.attention.proj_o.weight', 'h.20.ln_cross_attn.bias', 'h.11.crossattention.attention.proj_q.weight', 'h.14.ln_cross_attn.bias', 'h.23.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.23.crossattention.attn_gate', 'h.17.crossattention.attention.proj_o.weight', 'h.8.crossattention.attn_gate', 'h.20.crossattention.attention.proj_o.weight', 'h.14.crossattention.attention.proj_k.weight', 'h.23.crossattention.attention.proj_v.weight', 'h.8.crossattention.attention.proj_o.weight', 'h.8.ln_cross_attn.bias', 'h.5.ln_cross_attn.bias', 'h.20.crossattention.attention.proj_v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "from transformers import AutoConfig, AutoModel\n",
    "config = AutoConfig.from_pretrained(\"gpt2-medium\")\n",
    "config.know_proj_bias = False\n",
    "config.know_type = \"gated_cross\"\n",
    "config.know_pos = \"mlp\"\n",
    "config.know_layer = [5,8,11,14,17,20,23]\n",
    "config.hidden_dropout = 0.1\n",
    "\n",
    "enc_model2 = AutoModel.from_pretrained(\"roberta-base\")\n",
    "dec_model2 = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2498, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_states = enc_model2(batch[\"input_ids\"], batch[\"attention_mask\"]).last_hidden_state\n",
    "enc_states = nn.Linear(768,1024)(enc_states)\n",
    "dec_model2(input_ids = batch[\"decoder_input_ids\"], \n",
    "           attention_mask = batch[\"decoder_attention_mask\"], \n",
    "           encoder_hidden_states = enc_states, \n",
    "           labels = batch[\"labels\"], \n",
    "           encoder_attention_mask= batch[\"attention_mask\"]).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Daniel/.netrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['h.5.crossattention.attention.proj_k.weight', 'h.23.crossattention.attention.proj_o.weight', 'h.11.crossattention.attention.proj_o.weight', 'h.5.ln_cross_attn.weight', 'h.14.crossattention.attention.proj_v.weight', 'h.14.crossattention.attn_gate', 'h.20.crossattention.attention.proj_q.weight', 'h.11.crossattention.attention.proj_v.weight', 'h.17.crossattention.attention.proj_v.weight', 'h.11.crossattention.attn_gate', 'h.14.crossattention.attention.proj_q.weight', 'h.17.crossattention.attention.proj_q.weight', 'h.8.crossattention.attention.proj_k.weight', 'h.23.ln_cross_attn.weight', 'h.23.crossattention.attention.proj_k.weight', 'h.8.crossattention.attention.proj_v.weight', 'h.5.crossattention.attention.proj_v.weight', 'h.23.crossattention.attention.proj_q.weight', 'h.14.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias', 'h.20.crossattention.attention.proj_k.weight', 'h.11.crossattention.attention.proj_k.weight', 'h.17.ln_cross_attn.weight', 'h.5.crossattention.attn_gate', 'h.20.ln_cross_attn.weight', 'h.14.crossattention.attention.proj_o.weight', 'h.11.ln_cross_attn.weight', 'h.17.crossattention.attn_gate', 'h.20.crossattention.attn_gate', 'h.17.crossattention.attention.proj_k.weight', 'h.5.crossattention.attention.proj_q.weight', 'h.8.crossattention.attention.proj_q.weight', 'h.17.ln_cross_attn.bias', 'h.5.crossattention.attention.proj_o.weight', 'h.20.ln_cross_attn.bias', 'h.11.crossattention.attention.proj_q.weight', 'h.14.ln_cross_attn.bias', 'h.23.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.23.crossattention.attn_gate', 'h.17.crossattention.attention.proj_o.weight', 'h.8.crossattention.attn_gate', 'h.20.crossattention.attention.proj_o.weight', 'h.14.crossattention.attention.proj_k.weight', 'h.23.crossattention.attention.proj_v.weight', 'h.8.crossattention.attention.proj_o.weight', 'h.8.ln_cross_attn.bias', 'h.5.ln_cross_attn.bias', 'h.20.crossattention.attention.proj_v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer,AutoConfig,TrainingArguments\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "from transformers import EncoderDecoderConfig, AutoModel\n",
    "from eval_utils.loading_utils import load_encdec_model\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "from train_utils.utils import prepare_dataset, CustomCollator, prompt_qca, prompt_qa, prompt_article, prompt_article_summary, prompt_qc_enc\n",
    "from train_utils.eval import evaluate\n",
    "from train_utils.Trainer import CustomTrainer, UpdateOutputDirCallback, AdditionalEvalCallback\n",
    "\n",
    "\n",
    "wandb.login(key = \"f190694cef6354f5205256582202a2b16502a236\")\n",
    "args = OmegaConf.load(\"configs/default.yaml\")\n",
    "\n",
    "\n",
    "train_enc_dec = args.model_args.is_enc_dec\n",
    "freeze_decoder = args.model_args.freeze_decoder\n",
    "freeze_encoder = args.model_args.freeze_encoder\n",
    "create_labels = args.data_args.cover_labels\n",
    "encoder_model = args.model_args.encoder_name\n",
    "decoder_checkpoint = args.model_args.decoder_name\n",
    "if args.data_args.prompt_type == \"qc\":\n",
    "    prompt = prompt_qca\n",
    "elif args.data_args.prompt_type == \"q\":\n",
    "    prompt = prompt_qa\n",
    "elif args.data_args.prompt_type == \"article\":\n",
    "    prompt = prompt_article\n",
    "elif args.data_args.prompt_type == \"article_summary\":\n",
    "    prompt = prompt_article_summary\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(args.model_args.decoder_base_name, trust_remote_code = True)\n",
    "config.know_type = args.model_args.adapter_args.adapter_type\n",
    "config.enc_dim = args.model_args.adapter_args.enc_dim\n",
    "config.know_layer = OmegaConf.to_container(args.model_args.adapter_args.know_layer)\n",
    "config.hidden_dropout = args.model_args.adapter_args.hidden_dropout\n",
    "config.know_proj_bias = args.model_args.adapter_args.proj_bias\n",
    "config.know_pos = args.model_args.adapter_args.know_pos\n",
    "config.know_norm = args.model_args.adapter_args.know_norm\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_args.decoder_base_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.bos_token, tokenizer.bos_token_id),(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "\n",
    "if os.path.isdir(decoder_checkpoint) and isinstance((conf:=AutoConfig.from_pretrained(decoder_checkpoint)), EncoderDecoderConfig):\n",
    "    print(\"---- Loading Encoder Decoder Model ----\t\")\n",
    "    model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(decoder_checkpoint, enc_model_class= AutoModel, dec_model_class= GPT2LMHeadModel)\n",
    "else:\n",
    "    if \"phi\" in args.model_args.decoder_base_name:\n",
    "        from phi.modeling_phi import PhiForCausalLM\n",
    "        model = PhiForCausalLM.from_pretrained(decoder_checkpoint, config=config)\n",
    "    elif \"gpt2\" in args.model_args.decoder_base_name:\n",
    "        from modeling_gpt2 import GPT2LMHeadModel\n",
    "        model = GPT2LMHeadModel.from_pretrained(decoder_checkpoint, config=config)\n",
    "    else:\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        model = AutoModelForCausalLM.from_pretrained(decoder_checkpoint, config=config)\n",
    "\n",
    "    if train_enc_dec:\n",
    "        from transformers import EncoderDecoderConfig, AutoModel, AutoConfig, AutoTokenizer\n",
    "        from train_utils.EncoderDecoder import CustomEncoderDecoderModel\n",
    "\n",
    "        if os.path.isdir(encoder_model) and \"config_sentence_transformers.json\" in os.listdir(encoder_model):\n",
    "            print(\"---- loading Sentence Transformer Encoder ----\")\n",
    "            from train_utils.encoder import PrefixEncoder\n",
    "            enc_model, enc_tokenizer = PrefixEncoder.from_sentenc_checkpoint(encoder_model)\n",
    "        else:\n",
    "            enc_model = AutoModel.from_pretrained(encoder_model)\n",
    "            enc_tokenizer = AutoTokenizer.from_pretrained(encoder_model)\n",
    "\n",
    "\n",
    "        config = EncoderDecoderConfig(**{\"encoder\": enc_model.config.to_dict(), \"decoder\": AutoConfig.from_pretrained(\"gpt2\").to_dict()})\n",
    "        config.decoder= model.config\n",
    "        model = CustomEncoderDecoderModel(encoder=enc_model, decoder=model, config = config)\n",
    "    else:\n",
    "        enc_tokenizer = None\n",
    "\n",
    "if freeze_encoder:\n",
    "    for n,p in model.encoder.named_parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "if freeze_decoder:\n",
    "    for n, p in (model.decoder if hasattr(model, \"decoder\") else model).named_parameters():\n",
    "        if not \"proj_k\" in n and not \"proj_v\" in n and \"gated_attn\" not in n and \"cross\" not in n:\n",
    "            p.requires_grad = False\n",
    "\n",
    "model.args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 87599/87599 [00:10<00:00, 8029.70 examples/s]\n",
      "Map: 100%|██████████| 87599/87599 [00:21<00:00, 4123.17 examples/s]\n",
      "Map: 100%|██████████| 10570/10570 [00:02<00:00, 4201.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(args.data_args.dataset_path) if os.path.isdir(args.data_args.dataset_path) else load_dataset(args.data_args.dataset_path)\n",
    "if \"squad\" in args.data_args.dataset_path:\n",
    "    if args.data_args.context_column == \"answer_sentence\" and \"answer_sentence\" not in dataset[\"train\"].column_names:\n",
    "        from train_utils.utils import extract_sentence\n",
    "        dataset = dataset.map(lambda x: {\"answer_sentence\": extract_sentence(x[\"context\"], x[\"answers\"][\"answer_start\"][0])})\n",
    "\n",
    "    dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "    dataset = dataset.map(lambda x: {k:v.strip() for k,v in x.items()})\n",
    "elif \"cnn\" in args.data_args.dataset_path:\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "    dataset = dataset.map(lambda x: {\"summary\": [\" \".join(entry.split()[:100]) for entry in x[\"highlights\"]]}, batched=True)\n",
    "    dataset = dataset.map(lambda x: {\"article\": [\" \".join(entry.split()[:200]) for entry in x[\"article_half\"]]}, batched=True, remove_columns=column_names)\n",
    "\n",
    "df_qca = dataset.map(prepare_dataset, \n",
    "                     fn_kwargs={\"prompt\": prompt, \n",
    "                                \"tokenizer\": tokenizer, \n",
    "                                \"create_labels\" : create_labels, \n",
    "                                \"enc_tokenizer\": enc_tokenizer, \n",
    "                                \"context_enc\": train_enc_dec, \n",
    "                                \"context_column\": args.data_args.context_column,\n",
    "                                \"answer_column\": args.data_args.answer_column,\n",
    "                                \"enc_prompt\": prompt_qc_enc if args.data_args.context_column == \"qc\" else None,\n",
    "                                \"num_prefix_token\": model.encoder.num_prefix_token if train_enc_dec and hasattr(model.encoder, \"num_prefix_token\") else 0}, \n",
    "                     batched=True, \n",
    "                     remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(df_qca[\"train\"], batch_size=3, collate_fn=CustomCollator(tokenizer, enc_tokenizer = enc_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2498)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder(input_ids = batch[\"decoder_input_ids\"],\n",
    "              attention_mask = batch[\"decoder_attention_mask\"],\n",
    "              labels = batch[\"labels\"]).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_states = model.encoder(input_ids = batch[\"input_ids\"],\n",
    "              attention_mask = batch[\"attention_mask\"]).last_hidden_state\n",
    "enc_states = nn.Linear(768,1024)(enc_states)\n",
    "\n",
    "import torch \n",
    "\n",
    "enc_states_rand = torch.rand_like(enc_states)\n",
    "\n",
    "\n",
    "model.decoder(input_ids = batch[\"decoder_input_ids\"],\n",
    "            attention_mask = batch[\"decoder_attention_mask\"],\n",
    "            labels = batch[\"labels\"],\n",
    "            encoder_hidden_states = enc_states_rand,\n",
    "            encoder_attention_mask = batch[\"attention_mask\"]).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**batch).loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.transformer.h.5.crossattention.attn_gate\n",
      "tensor([-0.0132])\n",
      "decoder.transformer.h.8.crossattention.attn_gate\n",
      "tensor([0.0058])\n",
      "decoder.transformer.h.11.crossattention.attn_gate\n",
      "tensor([0.0018])\n",
      "decoder.transformer.h.14.crossattention.attn_gate\n",
      "tensor([-0.0041])\n",
      "decoder.transformer.h.17.crossattention.attn_gate\n",
      "tensor([0.0036])\n",
      "decoder.transformer.h.20.crossattention.attn_gate\n",
      "tensor([0.0002])\n",
      "decoder.transformer.h.23.crossattention.attn_gate\n",
      "tensor([-0.0030])\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if \"gate\" in n:\n",
    "        print(n)\n",
    "        print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils.loading_utils import load_encdec_model\n",
    "from transformers import AutoModel\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(\"checkpoints/checkpoint-5475_test_cros_only_gated/\", enc_model_class= AutoModel, dec_model_class= GPT2LMHeadModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_batch</th>\n",
       "      <th>loss_batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>generated</th>\n",
       "      <th>reference</th>\n",
       "      <th>answer_logits</th>\n",
       "      <th>gen_logits</th>\n",
       "      <th>adapter_attn</th>\n",
       "      <th>adapter_mean</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>...</th>\n",
       "      <th>solution_present</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>decoder_loss</th>\n",
       "      <th>decoder_generated</th>\n",
       "      <th>decoder_answer_logits</th>\n",
       "      <th>decoder_gen_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'decoder_input_ids': [50256, 13828, 5134, 107...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 13828, 5134, 107...</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>New England Patriots</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>[[9.86386, 9.707694, 6.367945, 4.290615, 6.433...</td>\n",
       "      <td>[[-83.98282, -84.14194, -87.02244, -89.86123, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.866573691368103], 'recall': [...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.963347</td>\n",
       "      <td>The New England Patriots.\\n\\nThe Patriots won...</td>\n",
       "      <td>[[-31.766266, -36.457012, -37.86325, -40.3044,...</td>\n",
       "      <td>[[-113.81351, -114.37096, -116.42429, -118.155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'decoder_input_ids': [50256, 13828, 5134, 107...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 13828, 5134, 107...</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>New Orleans Saints\\nAnswer: New Orleans Saint...</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>[[-7.804997, -9.273602, -13.947753, -14.553748...</td>\n",
       "      <td>[[-76.7976, -76.62437, -79.1884, -82.336716, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.7514113187789917], 'recall': ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.963347</td>\n",
       "      <td>The New England Patriots.\\n\\nThe Patriots won...</td>\n",
       "      <td>[[-35.658188, -40.345284, -42.434048, -43.9317...</td>\n",
       "      <td>[[-110.67171, -111.104675, -112.957855, -114.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'decoder_input_ids': [50256, 50256, 50256, 50...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 8496, 750, 3115,...</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>New Orleans\\nAnswer: Super Bowl 50 was held i...</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>[[-21.079342, -19.898584, -24.010008, -28.6909...</td>\n",
       "      <td>[[-53.4031, -52.51433, -57.01871, -57.266476, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.018181818181818...</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.7864809036254883], 'recall': ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.963347</td>\n",
       "      <td>In the United States.\\n\\nThe Super Bowl was h...</td>\n",
       "      <td>[[-69.505455, -72.853065, -77.54536, -79.49573...</td>\n",
       "      <td>[[-98.00317, -98.19375, -101.61287, -102.07279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'decoder_input_ids': [50256, 50256, 50256, 50...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 13828, 5134, 107...</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>New England Patriots\\n\\nAnswer: New England P...</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>[[1.0915244, 0.23584893, -2.9224572, -4.822131...</td>\n",
       "      <td>[[-78.3024, -77.840324, -81.18579, -84.52092, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.7575564384460449], 'recall': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.963347</td>\n",
       "      <td>The New England Patriots.\\n\\nThe Patriots won...</td>\n",
       "      <td>[[-40.301838, -44.63461, -46.536575, -48.8318,...</td>\n",
       "      <td>[[-117.03637, -117.29913, -119.36191, -121.301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'decoder_input_ids': [50256, 2061, 3124, 373,...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 2061, 3124, 373,...</td>\n",
       "      <td>1.075779</td>\n",
       "      <td>white</td>\n",
       "      <td>gold</td>\n",
       "      <td>[[13.914301, 13.181443, 10.802457, 7.491674, 8...</td>\n",
       "      <td>[[-78.75067, -79.23787, -78.86608, -83.4689, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.9015482664108276], 'recall': ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.473165</td>\n",
       "      <td>Black.\\n\\nWhat color was used to emphasize th...</td>\n",
       "      <td>[[-29.425053, -35.962685, -35.373478, -38.9149...</td>\n",
       "      <td>[[-97.693825, -98.63494, -98.22438, -100.67448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>{'decoder_input_ids': [50256, 50256, 50256, 50...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 818, 644, 614, 7...</td>\n",
       "      <td>1.871112</td>\n",
       "      <td>2011\\nAnswer: 2011\\nAnswer: 2011\\nAnswer: 201...</td>\n",
       "      <td>1978</td>\n",
       "      <td>[[38.77231, 38.684677, 35.875156, 31.979534, 3...</td>\n",
       "      <td>[[-157.15797, -156.95673, -160.17026, -160.888...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.6898300051689148], 'recall': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>3.407411</td>\n",
       "      <td>1999.\\n\\nThe NFL's first 16-game regular seas...</td>\n",
       "      <td>[[3.700407, -1.9532802, -2.7853441, -6.6790795...</td>\n",
       "      <td>[[-145.63051, -145.64934, -148.73933, -148.765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>{'decoder_input_ids': [50256, 8241, 550, 262, ...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 8241, 550, 262, ...</td>\n",
       "      <td>1.265450</td>\n",
       "      <td>Carolina Panthers\\nAnswer: Carolina Panthers\\...</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>[[-5.6393676, -6.2195373, -10.557196, -11.2515...</td>\n",
       "      <td>[[-87.54248, -87.23083, -89.15566, -92.85159, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.041666666666666...</td>\n",
       "      <td>{'rouge1': 0.10526315789473684, 'rouge2': 0.05...</td>\n",
       "      <td>{'precision': [0.74830561876297], 'recall': [0...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3.763394</td>\n",
       "      <td>The Packers.\\n\\nThe Packers had the best reco...</td>\n",
       "      <td>[[-59.192986, -63.50041, -66.40582, -67.6587, ...</td>\n",
       "      <td>[[-113.9354, -114.28802, -116.65331, -118.2434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>{'decoder_input_ids': [50256, 2437, 867, 17782...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 2437, 867, 17782...</td>\n",
       "      <td>1.265450</td>\n",
       "      <td>five\\nAnswer: five\\nAnswer: five\\nAnswer: fiv...</td>\n",
       "      <td>Ten</td>\n",
       "      <td>[[15.71775, 13.562393, 13.220857, 9.056921, 13...</td>\n",
       "      <td>[[-147.28789, -147.46153, -147.65422, -149.279...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.6993039846420288], 'recall': ...</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.763394</td>\n",
       "      <td>\\n\\nThe Panthers went to the Pro Bowl in each ...</td>\n",
       "      <td>[[-27.30269, -33.06101, -34.622677, -37.73731,...</td>\n",
       "      <td>[[-131.08069, -131.3189, -132.47438, -133.3770...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>{'decoder_input_ids': [50256, 2437, 867, 17782...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 2437, 867, 17782...</td>\n",
       "      <td>1.265450</td>\n",
       "      <td>five\\nAnswer: five\\nAnswer: five\\nAnswer: fiv...</td>\n",
       "      <td>eight</td>\n",
       "      <td>[[10.745259, 10.183949, 8.075109, 3.6677494, 8...</td>\n",
       "      <td>[[-144.49245, -144.50192, -144.66614, -146.638...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.6994197368621826], 'recall': ...</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>3.763394</td>\n",
       "      <td>\\n\\nThe Panthers designated 11 players to the ...</td>\n",
       "      <td>[[-55.743523, -60.843544, -62.542656, -66.7097...</td>\n",
       "      <td>[[-129.40627, -129.72078, -130.62915, -131.725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>{'decoder_input_ids': [50256, 2061, 30364, 261...</td>\n",
       "      <td>{'decoder_input_ids': [50256, 2061, 30364, 261...</td>\n",
       "      <td>1.265450</td>\n",
       "      <td>Michael Strahan\\nAnswer: Michael Strahan\\nAns...</td>\n",
       "      <td>Kelvin Benjamin</td>\n",
       "      <td>[[-74.26226, -72.293495, -77.58024, -74.87137,...</td>\n",
       "      <td>[[-105.16629, -105.29438, -105.92762, -108.536...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n",
       "      <td>{'precision': [0.7515822649002075], 'recall': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3.763394</td>\n",
       "      <td>He tore his ACL in the preseason.\\n\\nWhat Pan...</td>\n",
       "      <td>[[-101.65802, -102.001495, -106.738205, -104.8...</td>\n",
       "      <td>[[-107.16581, -106.48679, -108.31409, -109.060...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             gen_batch  \\\n",
       "0    {'decoder_input_ids': [50256, 13828, 5134, 107...   \n",
       "1    {'decoder_input_ids': [50256, 13828, 5134, 107...   \n",
       "2    {'decoder_input_ids': [50256, 50256, 50256, 50...   \n",
       "3    {'decoder_input_ids': [50256, 50256, 50256, 50...   \n",
       "4    {'decoder_input_ids': [50256, 2061, 3124, 373,...   \n",
       "..                                                 ...   \n",
       "199  {'decoder_input_ids': [50256, 50256, 50256, 50...   \n",
       "200  {'decoder_input_ids': [50256, 8241, 550, 262, ...   \n",
       "201  {'decoder_input_ids': [50256, 2437, 867, 17782...   \n",
       "202  {'decoder_input_ids': [50256, 2437, 867, 17782...   \n",
       "203  {'decoder_input_ids': [50256, 2061, 30364, 261...   \n",
       "\n",
       "                                            loss_batch      loss  \\\n",
       "0    {'decoder_input_ids': [50256, 13828, 5134, 107...  0.670158   \n",
       "1    {'decoder_input_ids': [50256, 13828, 5134, 107...  0.670158   \n",
       "2    {'decoder_input_ids': [50256, 8496, 750, 3115,...  0.670158   \n",
       "3    {'decoder_input_ids': [50256, 13828, 5134, 107...  0.670158   \n",
       "4    {'decoder_input_ids': [50256, 2061, 3124, 373,...  1.075779   \n",
       "..                                                 ...       ...   \n",
       "199  {'decoder_input_ids': [50256, 818, 644, 614, 7...  1.871112   \n",
       "200  {'decoder_input_ids': [50256, 8241, 550, 262, ...  1.265450   \n",
       "201  {'decoder_input_ids': [50256, 2437, 867, 17782...  1.265450   \n",
       "202  {'decoder_input_ids': [50256, 2437, 867, 17782...  1.265450   \n",
       "203  {'decoder_input_ids': [50256, 2061, 30364, 261...  1.265450   \n",
       "\n",
       "                                             generated  \\\n",
       "0                                 New England Patriots   \n",
       "1     New Orleans Saints\\nAnswer: New Orleans Saint...   \n",
       "2     New Orleans\\nAnswer: Super Bowl 50 was held i...   \n",
       "3     New England Patriots\\n\\nAnswer: New England P...   \n",
       "4                                                white   \n",
       "..                                                 ...   \n",
       "199   2011\\nAnswer: 2011\\nAnswer: 2011\\nAnswer: 201...   \n",
       "200   Carolina Panthers\\nAnswer: Carolina Panthers\\...   \n",
       "201   five\\nAnswer: five\\nAnswer: five\\nAnswer: fiv...   \n",
       "202   five\\nAnswer: five\\nAnswer: five\\nAnswer: fiv...   \n",
       "203   Michael Strahan\\nAnswer: Michael Strahan\\nAns...   \n",
       "\n",
       "                   reference  \\\n",
       "0             Denver Broncos   \n",
       "1          Carolina Panthers   \n",
       "2    Santa Clara, California   \n",
       "3             Denver Broncos   \n",
       "4                       gold   \n",
       "..                       ...   \n",
       "199                     1978   \n",
       "200        Carolina Panthers   \n",
       "201                      Ten   \n",
       "202                    eight   \n",
       "203          Kelvin Benjamin   \n",
       "\n",
       "                                         answer_logits  \\\n",
       "0    [[9.86386, 9.707694, 6.367945, 4.290615, 6.433...   \n",
       "1    [[-7.804997, -9.273602, -13.947753, -14.553748...   \n",
       "2    [[-21.079342, -19.898584, -24.010008, -28.6909...   \n",
       "3    [[1.0915244, 0.23584893, -2.9224572, -4.822131...   \n",
       "4    [[13.914301, 13.181443, 10.802457, 7.491674, 8...   \n",
       "..                                                 ...   \n",
       "199  [[38.77231, 38.684677, 35.875156, 31.979534, 3...   \n",
       "200  [[-5.6393676, -6.2195373, -10.557196, -11.2515...   \n",
       "201  [[15.71775, 13.562393, 13.220857, 9.056921, 13...   \n",
       "202  [[10.745259, 10.183949, 8.075109, 3.6677494, 8...   \n",
       "203  [[-74.26226, -72.293495, -77.58024, -74.87137,...   \n",
       "\n",
       "                                            gen_logits adapter_attn  \\\n",
       "0    [[-83.98282, -84.14194, -87.02244, -89.86123, ...         None   \n",
       "1    [[-76.7976, -76.62437, -79.1884, -82.336716, -...         None   \n",
       "2    [[-53.4031, -52.51433, -57.01871, -57.266476, ...         None   \n",
       "3    [[-78.3024, -77.840324, -81.18579, -84.52092, ...         None   \n",
       "4    [[-78.75067, -79.23787, -78.86608, -83.4689, -...         None   \n",
       "..                                                 ...          ...   \n",
       "199  [[-157.15797, -156.95673, -160.17026, -160.888...         None   \n",
       "200  [[-87.54248, -87.23083, -89.15566, -92.85159, ...         None   \n",
       "201  [[-147.28789, -147.46153, -147.65422, -149.279...         None   \n",
       "202  [[-144.49245, -144.50192, -144.66614, -146.638...         None   \n",
       "203  [[-105.16629, -105.29438, -105.92762, -108.536...         None   \n",
       "\n",
       "    adapter_mean  exact_match  ...  solution_present  \\\n",
       "0           None            0  ...                 0   \n",
       "1           None            0  ...                 0   \n",
       "2           None            0  ...                 0   \n",
       "3           None            0  ...                 0   \n",
       "4           None            0  ...                 0   \n",
       "..           ...          ...  ...               ...   \n",
       "199         None            0  ...                 0   \n",
       "200         None            0  ...                 1   \n",
       "201         None            0  ...                 0   \n",
       "202         None            0  ...                 0   \n",
       "203         None            0  ...                 0   \n",
       "\n",
       "                                                  bleu  \\\n",
       "0    {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "1    {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "2    {'bleu': 0.0, 'precisions': [0.018181818181818...   \n",
       "3    {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "4    {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "..                                                 ...   \n",
       "199  {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "200  {'bleu': 0.0, 'precisions': [0.041666666666666...   \n",
       "201  {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "202  {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "203  {'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0....   \n",
       "\n",
       "                                                 rouge  \\\n",
       "0    {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "1    {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "2    {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "3    {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "4    {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "..                                                 ...   \n",
       "199  {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "200  {'rouge1': 0.10526315789473684, 'rouge2': 0.05...   \n",
       "201  {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "202  {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "203  {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...   \n",
       "\n",
       "                                            bert_score id  batch_id  \\\n",
       "0    {'precision': [0.866573691368103], 'recall': [...  0         0   \n",
       "1    {'precision': [0.7514113187789917], 'recall': ...  1         0   \n",
       "2    {'precision': [0.7864809036254883], 'recall': ...  2         0   \n",
       "3    {'precision': [0.7575564384460449], 'recall': ...  3         0   \n",
       "4    {'precision': [0.9015482664108276], 'recall': ...  0         1   \n",
       "..                                                 ... ..       ...   \n",
       "199  {'precision': [0.6898300051689148], 'recall': ...  3        49   \n",
       "200  {'precision': [0.74830561876297], 'recall': [0...  0        50   \n",
       "201  {'precision': [0.6993039846420288], 'recall': ...  1        50   \n",
       "202  {'precision': [0.6994197368621826], 'recall': ...  2        50   \n",
       "203  {'precision': [0.7515822649002075], 'recall': ...  3        50   \n",
       "\n",
       "     decoder_loss                                  decoder_generated  \\\n",
       "0        2.963347   The New England Patriots.\\n\\nThe Patriots won...   \n",
       "1        2.963347   The New England Patriots.\\n\\nThe Patriots won...   \n",
       "2        2.963347   In the United States.\\n\\nThe Super Bowl was h...   \n",
       "3        2.963347   The New England Patriots.\\n\\nThe Patriots won...   \n",
       "4        3.473165   Black.\\n\\nWhat color was used to emphasize th...   \n",
       "..            ...                                                ...   \n",
       "199      3.407411   1999.\\n\\nThe NFL's first 16-game regular seas...   \n",
       "200      3.763394   The Packers.\\n\\nThe Packers had the best reco...   \n",
       "201      3.763394  \\n\\nThe Panthers went to the Pro Bowl in each ...   \n",
       "202      3.763394  \\n\\nThe Panthers designated 11 players to the ...   \n",
       "203      3.763394   He tore his ACL in the preseason.\\n\\nWhat Pan...   \n",
       "\n",
       "                                 decoder_answer_logits  \\\n",
       "0    [[-31.766266, -36.457012, -37.86325, -40.3044,...   \n",
       "1    [[-35.658188, -40.345284, -42.434048, -43.9317...   \n",
       "2    [[-69.505455, -72.853065, -77.54536, -79.49573...   \n",
       "3    [[-40.301838, -44.63461, -46.536575, -48.8318,...   \n",
       "4    [[-29.425053, -35.962685, -35.373478, -38.9149...   \n",
       "..                                                 ...   \n",
       "199  [[3.700407, -1.9532802, -2.7853441, -6.6790795...   \n",
       "200  [[-59.192986, -63.50041, -66.40582, -67.6587, ...   \n",
       "201  [[-27.30269, -33.06101, -34.622677, -37.73731,...   \n",
       "202  [[-55.743523, -60.843544, -62.542656, -66.7097...   \n",
       "203  [[-101.65802, -102.001495, -106.738205, -104.8...   \n",
       "\n",
       "                                    decoder_gen_logits  \n",
       "0    [[-113.81351, -114.37096, -116.42429, -118.155...  \n",
       "1    [[-110.67171, -111.104675, -112.957855, -114.8...  \n",
       "2    [[-98.00317, -98.19375, -101.61287, -102.07279...  \n",
       "3    [[-117.03637, -117.29913, -119.36191, -121.301...  \n",
       "4    [[-97.693825, -98.63494, -98.22438, -100.67448...  \n",
       "..                                                 ...  \n",
       "199  [[-145.63051, -145.64934, -148.73933, -148.765...  \n",
       "200  [[-113.9354, -114.28802, -116.65331, -118.2434...  \n",
       "201  [[-131.08069, -131.3189, -132.47438, -133.3770...  \n",
       "202  [[-129.40627, -129.72078, -130.62915, -131.725...  \n",
       "203  [[-107.16581, -106.48679, -108.31409, -109.060...  \n",
       "\n",
       "[204 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_pickle(\"checkpoints/eval_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.transformer.h.5.crossattention.attn_gate\n",
      "Parameter containing:\n",
      "tensor([-0.0336], requires_grad=True)\n",
      "decoder.transformer.h.8.crossattention.attn_gate\n",
      "Parameter containing:\n",
      "tensor([-0.0369], requires_grad=True)\n",
      "decoder.transformer.h.11.crossattention.attn_gate\n",
      "Parameter containing:\n",
      "tensor([-0.0686], requires_grad=True)\n",
      "decoder.transformer.h.14.crossattention.attn_gate\n",
      "Parameter containing:\n",
      "tensor([0.0792], requires_grad=True)\n",
      "decoder.transformer.h.17.crossattention.attn_gate\n",
      "Parameter containing:\n",
      "tensor([-0.0314], requires_grad=True)\n",
      "decoder.transformer.h.20.crossattention.attn_gate\n",
      "Parameter containing:\n",
      "tensor([-0.0364], requires_grad=True)\n",
      "decoder.transformer.h.23.crossattention.attn_gate\n",
      "Parameter containing:\n",
      "tensor([0.0500], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if \"gate\" in n:\n",
    "        print(n)\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 179, 1024])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec loss 2.964246988296509\n",
      "enc dec loss 2.964246988296509\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "proj_layer = nn.Linear(768, 1024)\n",
    "\n",
    "enc_states = enc_model(input_ids = loss_batch[\"input_ids\"], attention_mask = loss_batch[\"attention_mask\"]).last_hidden_state\n",
    "\n",
    "enc_states = proj_layer(enc_states)\n",
    "\n",
    "out = model(input_ids = loss_batch[\"decoder_input_ids\"], \n",
    "            attention_mask = loss_batch[\"decoder_attention_mask\"], \n",
    "            encoder_hidden_states = enc_states, \n",
    "            encoder_attention_mask = loss_batch[\"attention_mask\"],\n",
    "            labels = loss_batch[\"labels\"])\n",
    "\n",
    "dec_loss = model(input_ids = loss_batch[\"decoder_input_ids\"], \n",
    "            attention_mask = loss_batch[\"decoder_attention_mask\"], \n",
    "            labels = loss_batch[\"labels\"]).loss\n",
    "\n",
    "print(f\"dec loss {dec_loss}\")\n",
    "print(f\"enc dec loss {out.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "from transformers import AutoModel\n",
    "\n",
    "enc_model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "test = tokenizer(\"This is a test\", return_tensors = \"pt\")\n",
    "\n",
    "test_enc = enc_tokenizer(\"This is a test\", return_tensors = \"pt\")\n",
    "enc_states = enc_model(**test_enc).last_hidden_state\n",
    "\n",
    "from torch import nn\n",
    "proj_test  = nn.Linear(768,1024)\n",
    "\n",
    "enc_states = proj_test(enc_states)\n",
    "\n",
    "\n",
    "model(**test, encoder_hidden_states = enc_states, encoder_attention_mask = test_enc[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from phi.adapters import SharedAttention\n",
    "\n",
    "\n",
    "\n",
    "attn = SharedAttention(config)\n",
    "\n",
    "hidden_dim= 1024\n",
    "feed_forward_hidden_states = torch.rand(5,6,hidden_dim)\n",
    "cross_attn_out = torch.rand(5,6,hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn(feed_forward_hidden_states, cross_attn_out)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_positions = 512\n",
    "bias = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                1, 1, max_positions, max_positions)\n",
    "\n",
    "\n",
    "def apply_causal_mask(attn_scores):\n",
    "    # From GPT2 attention\n",
    "    query_length, key_length = attn_scores.size(-2), attn_scores.size(-1)\n",
    "    causal_mask = bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "    mask_value = torch.finfo(attn_scores.dtype).min\n",
    "    # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "    # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "    mask_value = torch.full([], mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n",
    "    attn_scores = torch.where(causal_mask, attn_scores.to(attn_scores.dtype), mask_value)\n",
    "    return attn_scores\n",
    "\n",
    "def _split_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Splits hidden_size dim into attn_head_size and num_heads\n",
    "    \"\"\"\n",
    "    new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "    tensor = tensor.view(new_shape)\n",
    "    return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "def _merge_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "    \"\"\"\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "    return tensor.view(new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_scores[0,0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat([feed_forward_hidden_states, cross_attn_out], dim = -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bert_score.apply(lambda x: x[\"f1\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/AIPHES/DiscoScore.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco_score import DiscoScorer\n",
    "\n",
    "disco_scorer = DiscoScorer(device='cpu', model_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = [\"Paul Merson has restarted his row with andros townsend after the Tottenham midfielder was brought on with only seven minutes remaining in his team 's 0-0 draw with burnley. Townsend was brought on in the 83rd minute for Tottenham as they drew 0-0 against Burnley .\"]\n",
    "\n",
    "references = [[\"Paul Merson has restarted his row with burnley on sunday. Townsend was brought on in the 83rd minute for tottenham. Andros Townsend scores england 's equaliser in their 1-1 friendly draw. Townsend hit a stunning equaliser for england against italy.\"]]\n",
    "references = [system]\n",
    "for s, refs in zip(system, references):\n",
    "   s = s.lower()\n",
    "   refs = [r.lower() for r in refs]\n",
    "   print(disco_scorer.DS_Focus_NN(s, refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder(input_ids = torch.tensor([[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]]), attention_mask = torch.tensor([[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]])).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = enc_tokenizer([\"hellow world this is a test\", \"this is a test\"], return_tensors = \"pt\", padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.num_prefix_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_input = {\"input_ids\": torch.tensor([[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]]), \"attention_mask\": torch.tensor([[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]])}\n",
    "test_input2 = {\"decoder_input_ids\": torch.tensor([[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]]), \"decoder_attention_mask\": torch.tensor([[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]])}\n",
    "model(**test_input, **test_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.encoder.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PrefixEncoder(\"checkpoints/2725_prefix10/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"token_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.enc_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils.loading_utils import load_encdec_model\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(\"checkpoints/checkpoint-10950_589/\", AutoModel, GPT2LMHeadModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer.pad_token = dec_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "loss_batch, gen_batch, answers = batch\n",
    "out = model.generate(**gen_batch, max_new_tokens = 30, eos_token_id = dec_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "prompt = \"\"\"Write a detailed analogy between mathematics and a lighthouse.\"\"\"\n",
    "input_example = dec_tokenizer(prompt, return_tensors = \"pt\")\n",
    "\n",
    "import torch\n",
    "\n",
    "encoder_states = torch.rand(1,5,1024)\n",
    "encoder_attn_mask = torch.ones(1,5).long()\n",
    "\n",
    "model(**input_example, encoder_hidden_states = encoder_states, encoder_attention_mask = encoder_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"squad_with_answer_sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"msmacro_wellformed\")\n",
    "\n",
    "split =dataset[\"train\"].train_test_split(test_size = 30_000, seed = 42)\n",
    "\n",
    "dataset_split = DatasetDict({\"train\": split[\"train\"], \"validation\": dataset[\"dev\"], \"test\": split[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split.save_to_disk(\"msmacro_wellformed_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.utils import prepare_dataset, prompt_q\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.bos_token, tokenizer.bos_token_id),(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "dataset = load_from_disk(\"squad_with_answer_sentence\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "dataset = dataset.map(lambda x: {k:v.strip() for k,v in x.items()})\n",
    "\n",
    "df_qca = dataset.map(prepare_dataset, \n",
    "                     fn_kwargs={\"prompt\": prompt_q, \n",
    "                                \"tokenizer\": tokenizer, \n",
    "                                \"create_labels\" : False, \n",
    "                                \"enc_tokenizer\": None, \n",
    "                                \"context_enc\": False, \n",
    "                                \"context_column\": \"context\"}, \n",
    "                     batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from train_utils.eval import EvalCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "checkpoint = \"checkpoints/gpt2_run_cosmic-butterfly-490/checkpoint-5475/\"\n",
    "mode = \"q\"\n",
    "checkpoint = \"checkpoints/gpt2_run_bumbling-energy-489/checkpoint-5475/\"\n",
    "mode = \"qc\"\n",
    "checkpoint = \"checkpoints/gpt2_q_msmacro_run_amber-donkey-491/checkpoint-7659/\"\n",
    "mode = \"q\"\n",
    "checkpoint = \"checkpoints/gpt2_qc_msmacro_run_solar-sponge-492/checkpoint-7659/\"\n",
    "mode = \"qc\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.bos_token, tokenizer.bos_token_id),(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "\n",
    "\n",
    "dataset = load_from_disk(\"msmacro_wellformed_split\")\n",
    "#dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "#dataset = dataset.map(lambda x: {k:v.strip() for k,v in x.items()})\n",
    "\n",
    "collate_fn = EvalCollator(tokenizer, None, mode=mode, context_enc = False, cover_labels=False)\n",
    "\n",
    "loader = DataLoader(dataset[\"validation\"], batch_size = 2, collate_fn = collate_fn)\n",
    "\n",
    "loss_batch, gen_batch, answers = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(**gen_batch, max_new_tokens = 30, eos_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"cnn_processed_50_150_300_max_all_cols\")\n",
    "\n",
    "column_names = dataset[\"train\"].column_names\n",
    "dataset = dataset.map(lambda x: {\"summary\": [\" \".join(entry.split()[:100]) for entry in x[\"highlights\"]]}, batched=True)\n",
    "dataset = dataset.map(lambda x: {\"article\": [\" \".join(entry.split()[:200]) for entry in x[\"article_half\"]]}, batched=True, remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"msmacro_wellformed_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "predictions = [\"hello there\", \"general konobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "results_bleu = bleu.compute(predictions=predictions, references=references)\n",
    "results_rouge = rouge.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"cnn_processed_50_150_300_max_all_cols\")\n",
    "dataset = dataset.remove_columns([\"input_ids\", \"enc_input_ids\"])\n",
    "dataset = dataset.map(lambda x: {\"article_init\": [\" \".join(entry.split()[:30]) for entry in x[\"article\"]]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"cnn_processed_50_150_300_max_init_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.eval import EvalCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"cnn_processed_50_150_300_max_init_gen\")\n",
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"article_init\")\n",
    "dataset = dataset.map(lambda x: {\"summary\": [\" \".join(entry.split()[:100]) for entry in x[\"highlights\"]]}, batched=True)\n",
    "dataset = dataset.map(lambda x: {\"article\": [\" \".join(entry.split()[:200]) for entry in x[\"article_half\"]]}, batched=True, remove_columns=column_names)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "collate_fn = EvalCollator(tokenizer, None, mode = \"article\", answer_column=\"article\")\n",
    "loader = DataLoader(dataset[\"train\"], batch_size = 100, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.utils import prepare_dataset, prompt_q, prompt_article\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"cnn_processed_50_150_300_max_init_gen\")\n",
    "column_names = dataset[\"train\"].column_names\n",
    "dataset = dataset.map(lambda x: {\"summary\": [\" \".join(entry.split()[:100]) for entry in x[\"highlights\"]]}, batched=True)\n",
    "dataset = dataset.map(lambda x: {\"article\": [\" \".join(entry.split()[:200]) for entry in x[\"article_half\"]]}, batched=True, remove_columns=column_names)\n",
    "\n",
    "df_qca = dataset.map(prepare_dataset, \n",
    "                     fn_kwargs={\"prompt\": prompt_article, \n",
    "                                \"tokenizer\": tokenizer, \n",
    "                                \"create_labels\" : False, \n",
    "                                \"enc_tokenizer\": None, \n",
    "                                \"context_enc\": False, \n",
    "                                \"context_column\": \"summary\",\n",
    "                                \"answer_column\": \"article\"}, \n",
    "                     batched=True, \n",
    "                     remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Indices resulting in really long input sequences\n",
    "#indices_to_drop = [60486, 69092, 98277, 157444, 173621]\n",
    "\n",
    "#def filter_indices(row, index):\n",
    "#    return index not in indices_to_drop\n",
    "\n",
    "#df_qca = df_qca.filter(filter_indices, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from train_utils.eval import  evaluate\n",
    "from phi.modeling_phi import PhiForCausalLM\n",
    "\n",
    "CHECKPOINT_FOLDER = \"checkpoints\"\n",
    "CHECKPOINT_PATHS = [\"phi_qc_squad_run_treasured-salad-497\"]\n",
    "OUTPUT_FOLDER = \"\"\n",
    "\n",
    "for checkpoint in CHECKPOINT_PATHS:\n",
    "    full_path = glob(os.path.join(CHECKPOINT_FOLDER, checkpoint,\"checkpoint-*/\"))[0]\n",
    "    args = OmegaConf.load(os.path.join(full_path, \"model_config.yaml\"))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_args.decoder_base_name)\n",
    "    enc_tokenizer = AutoTokenizer.from_pretrained(args.model_args.encoder_name) if args.model_args.is_enc_dec else None\n",
    "\n",
    "    if \"phi\" in args.model_args.decoder_base_name:\n",
    "        model = PhiForCausalLM.from_pretrained(full_path)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(full_path)\n",
    "\n",
    "    eval_result = evaluate(model = model, \n",
    "                          tokenizer = tokenizer,\n",
    "                          enc_tokenizer = enc_tokenizer,\n",
    "                          dataset_path = args.data_args.dataset_path,\n",
    "                          prompt_type = args.data_args.prompt_type,\n",
    "                          context_enc = args.model_args.is_enc_dec,\n",
    "                          cover_labels = args.data_args.cover_labels,\n",
    "                          context_column=args.data_args.context_column,\n",
    "                          answer_column = args.data_args.answer_column if \"answer_column\" in args.data_args else \"answers\",\n",
    "                          run_decoder_only = args.model_args.is_enc_dec,\n",
    "                          max_batches = -1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_args.answer_column if \"answer_column\" in args.data_args else \"answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco_score import DiscoScorer\n",
    "\n",
    "disco_scorer = DiscoScorer(device='cpu', model_name='bert-base-uncased')\n",
    "print(disco_scorer.DS_Focus_NN(s, refs)) # FocusDiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.utils import prepare_dataset, prompt_article, prompt_article_summary\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.bos_token, tokenizer.bos_token_id),(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "\n",
    "df_qca = dataset[\"train\"].map(prepare_dataset, fn_kwargs={\"prompt\": prompt_article, \n",
    "                                                 \"tokenizer\": tokenizer, \n",
    "                                                 \"create_labels\" : False, \n",
    "                                                 \"enc_tokenizer\": tokenizer, \n",
    "                                                 \"context_enc\": True, \n",
    "                                                 \"context_column\": \"summary\",\n",
    "                                                 \"answer_column\": \"article\",\n",
    "                                                 \"apply_tokenization\":True}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices resulting in really long input sequences\n",
    "indices_to_drop = [60486, 69092, 98277, 157444, 173621]\n",
    "\n",
    "def filter_indices(row, index):\n",
    "    return index not in indices_to_drop\n",
    "\n",
    "filtered_dataset = df_qca.filter(filter_indices, with_indices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df_qca.to_pandas()\n",
    "df_pd[\"len\"] = df_pd.decoder_input_ids.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_pd[df_pd.len > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(subset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qca.to_pandas().decoder_input_ids.apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(df_qca.to_pandas().input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_qca.to_pandas().text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qca.to_pandas().input_ids.apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation\"].to_pandas().article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qca.to_pandas()[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(tokenizer.batch_decode(df_qca.to_pandas()[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.modeling_phi import PhiForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = PhiForCausalLM.from_pretrained(\"checkpoints/phi_qc_squad_run_treasured-salad-497/checkpoint-5475/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.eval import evaluate\n",
    "\n",
    "out = evaluate(model, tokenizer, enc_tokenizer =None, dataset_path = \"squad\", prompt_type = \"qc\", batch_size = 1, max_batches = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd[\"id_len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd[df_pd.id_len >500].iloc[0].question.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.eval import EvalCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "eval_collator = EvalCollator(tokenizer, enc_tokenizer, mode = \"q\", context_enc=True, cover_labels=True)\n",
    "\n",
    "eval_loader = DataLoader(dataset_split[\"validation\"], batch_size = 1, collate_fn = eval_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch, gen_batch, answers = next(iter(eval_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Implementations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.configuration_phi import PhiConfig\n",
    "from phi.modeling_phi import PhiForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "config = PhiConfig(n_layer=3)\n",
    "config.know_type = \"kformer\"\n",
    "config.enc_dim = 1024\n",
    "config.know_layer = [0, 1, 2]\n",
    "\n",
    "dummy_model = PhiForCausalLM(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "enc = AutoModel.from_pretrained(\"roberta-base\")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, AutoModel, EncoderDecoderConfig, AutoConfig\n",
    "\n",
    "conf = EncoderDecoderConfig(**{\"encoder\": enc.config.to_dict(), \"decoder\": AutoConfig.from_pretrained(\"gpt2\").to_dict()})\n",
    "conf.decoder= dummy_model.config\n",
    "\n",
    "model = EncoderDecoderModel(encoder = enc, decoder= dummy_model, config=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(\"checkpoints\\checkpoint-5475_encdec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"test_enc_dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import prepare_dataset, prompt_qa\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "\n",
    "df_qa = dataset[\"validation\"].map(prepare_dataset, \n",
    "                     fn_kwargs={\"prompt\": prompt_qa, \"tokenizer\": tokenizer, \"create_labels\" : True, \"enc_tokenizer\": enc_tokenizer, \"context_enc\": True}, \n",
    "                     batched=True, \n",
    "                     remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import CustomCollator\n",
    "\n",
    "loader = DataLoader(df_qa, batch_size=2, collate_fn= CustomCollator(dec_tokenizer= tokenizer, enc_tokenizer= enc_tokenizer))\n",
    "batch = next(iter(loader))\n",
    "\n",
    "#model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model(input_ids = dec_input[\"input_ids\"], \n",
    "            encoder_hidden_states=enc_hidden_state, \n",
    "            encoder_attention_mask=enc_input[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Squad dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_qca = \"\"\"\\\n",
    "{context}\n",
    "{question}\n",
    "\n",
    "Answer: {answers}\"\"\"\n",
    "\n",
    "prompt_qc  = \"\"\"\\\n",
    "{context}\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_qa = \"\"\"\\\n",
    "{question}\n",
    "\n",
    "Answer: {answers}\"\"\"\n",
    "\n",
    "def prepare_dataset(examples, prompt, tokenizer, create_labels = False, return_answers = False):\n",
    "    input_text = [prompt.format(question = q, context = c, answers = a) for q, c, a in zip(examples[\"question\"], examples[\"context\"], examples[\"answers\"])]\n",
    "    input_ids = tokenizer(input_text, return_attention_mask=False)\n",
    "    answer_ids = tokenizer(examples[\"answers\"], return_attention_mask=False)[\"input_ids\"]\n",
    "    if create_labels:\n",
    "        labels = input_ids[\"input_ids\"].copy()\n",
    "        labels = [(len(l)-(len(a)))*[-100] + l[-len(a):] for l, a in zip(labels, answer_ids)]\n",
    "        input_ids.update({\"labels\": labels})\n",
    "    if return_answers:\n",
    "        input_ids.update({\"answer\": examples[\"answers\"]})\n",
    "    return input_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qca = dataset.map(prepare_dataset, fn_kwargs={\"prompt\": prompt_qca, \"tokenizer\": tokenizer, \"create_labels\" : True}, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "df_qa = dataset.map(prepare_dataset, fn_kwargs={\"prompt\": prompt_qa, \"tokenizer\": tokenizer, \"create_labels\" : True}, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class CustomCollator:\n",
    "    def __init__(self, dec_tokenizer,enc_tokenizer = None):\n",
    "        self.enc_pad_token_id = enc_tokenizer.pad_token_id if enc_tokenizer is not None else None\n",
    "        self.dec_pad_token_id = dec_tokenizer.pad_token_id\n",
    "        self.IGNORE_INDEX = -100\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids, labels, attention_mask  = None, None, None\n",
    "        # Extract and pad sequences for each column\n",
    "        if \"input_ids\" in batch[0]:\n",
    "            input_ids = pad_sequence([torch.tensor(item['input_ids']) for item in batch], batch_first=True, padding_value = self.dec_pad_token_id)\n",
    "            if \"labels\" in batch[0]:\n",
    "                labels = pad_sequence([torch.tensor(item['labels']) for item in batch], batch_first=True, padding_value = self.IGNORE_INDEX)\n",
    "            else:\n",
    "                labels = pad_sequence([torch.tensor(item['input_ids']) for item in batch], batch_first=True, padding_value = self.IGNORE_INDEX)\n",
    "            attention_mask = input_ids.ne(self.dec_pad_token_id)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \n",
    "                \"attention_mask\" : attention_mask, \n",
    "                \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader= DataLoader(df_qca[\"train\"], batch_size=2, collate_fn=CustomCollator(tokenizer))\n",
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Script\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# paged_adamw_8bit\n",
    "\n",
    "training_args= TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=\"/netscratch/roeder/phi_train\",\n",
    "    optim=\"adamw_bnb_8bit\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=dummy_model,\n",
    "    args=training_args,\n",
    "    train_dataset=df_qca[\"train\"],\n",
    "    eval_dataset=df_qca[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=CustomCollator(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.modeling_phi import PhiForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "model_path = \"checkpoints/checkpoint-5475_boseos/\"\n",
    "model_path = \"checkpoints/checkpoint-5475_boseos_qa_full/\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code = True)\n",
    "config.know_type = \"gated_cross_attn\"\n",
    "config.enc_dim = 2048\n",
    "config.know_layer =[5,8,11,14,17,20,23]\n",
    "config.know_proj_bias = False\n",
    "\n",
    "model = PhiForCausalLM.from_pretrained(model_path, config = config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.bos_token, tokenizer.bos_token_id),(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in model.transformer.named_parameters():\n",
    "    if \"gated_attn\" not in n:\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids = loss_batch[\"decoder_input_ids\"], attention_mask = loss_batch[\"decoder_attention_mask\"], encoder_hidden_states = torch.rand(3,10,2048), encoder_attention_mask = torch.ones(3,10), labels = loss_batch[\"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ms_marco\", \"v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.h[8].gated_attn.attn_gate.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.EncoderDecoder import CustomEncoderDecoderModel\n",
    "\n",
    "#checkpoint_path = \"checkpoints\\checkpoint-2737\"\n",
    "#checkpoint_path = \"checkpoints\\checkpoint-8211_enc_dec_fullprecsion-1e-4_8batch_4gradacc\"\n",
    "#model_path = \"checkpoints/checkpoint-5000_low_loss_polar_dragon/\"\n",
    "checkpoint_path= \"checkpoints/run_scarlet-bee-459/checkpoint-5475\"\n",
    "checkpoint_path = \"checkpoints/run_leafy-plasma-470/checkpoint-5475/\"\n",
    "\n",
    "model = CustomEncoderDecoderModel.from_pretrained(checkpoint_path)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.bos_token, tokenizer.bos_token_id),(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "\n",
    "for n, [p in model.decoder.named_parameters():\n",
    "    if not \"proj_k\" in n and not \"proj_v\" in n:\n",
    "        p.requires_grad = False\n",
    "\n",
    "from train_utils.eval import EvalCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset[\"validation\"].map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "loader = DataLoader(dataset, batch_size = 3, collate_fn = EvalCollator(tokenizer,enc_tokenizer, mode = \"q\", context_enc = True, cover_labels = True, context_column = \"answers\"))\n",
    "\n",
    "loss_batch, gen_batch, answers = next(iter(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.encoder(loss_batch[\"input_ids\"], attention_mask = loss_batch[\"attention_mask\"])[0]\n",
    "\n",
    "out = model.enc_to_dec_proj(hidden_states)\n",
    "v = model.decoder.transformer.h[11].proj_v(out)\n",
    "k = model.decoder.transformer.h[11].proj_k(out)\n",
    "print(hidden_states.mean())\n",
    "print(out.mean())\n",
    "print(v.mean())\n",
    "print(k.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.proj_q = nn.Linear(config.enc_dim, config.n_embd, bias= config.know_proj_bias)\n",
    "        self.proj_k = nn.Linear(config.enc_dim, config.n_embd, bias= config.know_proj_bias)\n",
    "        self.proj_v = nn.Linear(config.enc_dim, config.n_embd, bias= config.know_proj_bias)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "\n",
    "    def forward(self, query, key, value, attention_mask=None):\n",
    "        query = self.proj_q(query)\n",
    "        key = self.proj_k(key)\n",
    "        value = self.proj_v(value) \n",
    "\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "        attn_weights = attn_weights / torch.full(\n",
    "            [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
    "        attn_weights = attn_weights.type(value.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "class GatedCrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
    "        self.attention = Attention(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask\n",
    "    ):\n",
    "        attn_out, attn_weights = self.attention(hidden_states, encoder_hidden_states, encoder_hidden_states, encoder_attention_mask)\n",
    "\n",
    "        hidden_states = attn_out * self.attn_gate.tanh() + hidden_states\n",
    "\n",
    "        return hidden_states, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.adapters import GatedCrossAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_pickle(\"checkpoints/run_leafy-plasma-470/eval_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_to_dec_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hidden_states[0][0] - hidden_states[0][4]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.transformer.h[11].proj_v.bias is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.encoder(loss_batch[\"input_ids\"], attention_mask = loss_batch[\"attention_mask\"])[0][0][1] - model.encoder(loss_batch[\"input_ids\"], attention_mask = loss_batch[\"attention_mask\"])[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.transformer.h[0].mixer.Wqkv.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.eval import evaluate\n",
    "\n",
    "output = evaluate(model, \n",
    "                  tokenizer, \n",
    "                  enc_tokenizer, \n",
    "                  prompt_type = \"q\", \n",
    "                  context_enc = True, \n",
    "                  cover_labels = True, \n",
    "                  context_column = \"answers\", \n",
    "                  run_decoder_only = True,\n",
    "                  max_batches = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "from train_utils.eval import EvalCollator\n",
    "\n",
    "loader = DataLoader(dataset[\"validation\"], batch_size= 4, collate_fn= EvalCollator(tokenizer, enc_tokenizer, mode = \"q\", context_enc=True,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch, gen_batch, answers = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_batch = {k.replace(\"decoder_\",\"\"):v for k,v in loss_batch.items() if \"decoder\" in k or \"labels\" in k}\n",
    "decoder_gen_batch = {k.replace(\"decoder_\",\"\"):v for k,v in gen_batch.items() if \"decoder\" in k or \"labels\" in k}\n",
    "decoder_gen_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.decoder(**decoder_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the logits for only the answer tokens\n",
    "len_answers = [len(t) for t in tokenizer(answers).input_ids]\n",
    "answer_logits = [logits[-l_answer:,:] for logits, l_answer in zip(out.logits, len_answers)]\n",
    "\n",
    "answer_logits[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import prepare_dataset, prompt_qc, prompt_q, prompt_qa, CustomCollator\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "samples_qc = dataset[\"validation\"].map(prepare_dataset, \n",
    "                                       fn_kwargs={\"prompt\": prompt_qc, \"tokenizer\": tokenizer, \"apply_tokenization\":False}, \n",
    "                                       batched=True, \n",
    "                                       remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "samples_q_c = dataset[\"validation\"].map(prepare_dataset, \n",
    "                                       fn_kwargs={\"prompt\": prompt_qa, \"tokenizer\": tokenizer, \"apply_tokenization\":True, \"enc_tokenizer\": enc_tokenizer, \"context_enc\": True, \"create_labels\": False}, \n",
    "                                       batched=True, \n",
    "                                       remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(samples_q_c, batch_size = 1, collate_fn=CustomCollator(tokenizer, enc_tokenizer= enc_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder only\n",
    "out = model.decoder.generate(input_ids = batch[\"decoder_input_ids\"][:,:-3], max_new_tokens = 3, eos_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dec_ids_to_gen_input(tensor):\n",
    "    \"\"\"Cuts of the answer and applies left side padding\"\"\"\n",
    "    _,final_ids = torch.where(tensor == 25)\n",
    "    max_length = max(final_ids)\n",
    "    samples = []\n",
    "    for token_ids, max_id in zip(tensor,final_ids):\n",
    "        left_padding = torch.tensor([50256] * (max_length -max_id), dtype = token_ids.dtype)\n",
    "        input_tokens = token_ids[:max_id+1]\n",
    "        token_ids = torch.concat([left_padding, input_tokens])\n",
    "        samples.append(token_ids)\n",
    "    return torch.stack(samples)\n",
    "\n",
    "\n",
    "batch = next(iter(loader))\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "labels[labels == -100] = 50256\n",
    "answers = [label.split(\"Answer:\")[1].replace(\"<|endoftext|>\",\"\") for label in tokenizer.batch_decode(labels)]\n",
    "\n",
    "model.eval()\n",
    "out = model.generate(input_ids=batch[\"input_ids\"], decoder_input_ids = dec_ids_to_gen_input(batch[\"decoder_input_ids\"]), max_new_tokens=30, eos_token_id = tokenizer.eos_token_id)\n",
    "print(f\"Loss: {model(**batch).loss}\")\n",
    "print(f\"Prompt: {tokenizer.batch_decode(batch['decoder_input_ids'][:,:-3])}\")\n",
    "print(f\"Generated Text: {tokenizer.batch_decode(out, skip_special_tokens=True)}\")\n",
    "print(f\"Answers:  {answers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_enc_dec = samples_q_c.to_pandas().iloc[0].to_dict()\n",
    "decoder_input_ids = list(input_enc_dec[\"decoder_input_ids\"])\n",
    "decoder_input_ids.insert(0, tokenizer.bos_token_id)\n",
    "import torch \n",
    "input_enc_dec = {\"decoder_input_ids\" :torch.tensor(decoder_input_ids).unsqueeze(0), \"input_ids\": torch.tensor(input_enc_dec[\"input_ids\"]).unsqueeze(0)}\n",
    "\n",
    "output = model.generate(input_ids = input_enc_dec[\"input_ids\"], decoder_input_ids = input_enc_dec[\"decoder_input_ids\"], max_new_tokens=30, eos_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task = \"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_new_tokens = 30, \n",
    "                return_full_text = False, \n",
    "                stop_sequence= tokenizer.eos_token,\n",
    "                prefix = tokenizer.eos_token,\n",
    "                batch_size = 2)\n",
    "\n",
    "input_text = samples_qc[\"text\"]\n",
    "labels = samples_qc[\"answer\"]\n",
    "\n",
    "outputs = pipe(input_text[:5])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(tokenizer(input_text[0], return_tensors=\"pt\")[\"input_ids\"][:,1:-1], max_new_tokens=30, eos_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train_eval_phi_output_new.csv\")\n",
    "df2 = pd.read_csv(\"train_eval_phi_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFKI LM benchmarking prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "splits = ['xquad.ar', 'xquad.de', 'xquad.zh', 'xquad.vi', 'xquad.en', 'xquad.es', 'xquad.hi', 'xquad.el', 'xquad.th', 'xquad.tr', 'xquad.ru', 'xquad.ro']\n",
    "dataset = load_dataset(\"xquad\", splits[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = dataset[\"validation\"].to_pandas()\n",
    "df_pd.answers = df_pd.apply(lambda x: x[\"answers\"][\"text\"][0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_pd.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\\\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "llama2_prompt = \"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_msg_1} [/INST] {model_answer_1} </s><s>[INST] {user_msg_2} [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Your are a helpful assitant that extracts answers from a context passage given a question.\"\n",
    "user_msg_1 = user_prompt.format(context = sample.context, question = sample.question)\n",
    "model_answer = sample.answers\n",
    "user_msg2 = user_prompt.format(context = \"blub\", question = \"blub\")\n",
    "\n",
    "\n",
    "llama2_prompt = llama2_prompt.format(system_prompt = system_prompt, user_msg_1 = user_msg_1, model_answer_1 = model_answer, user_msg_2 = user_msg2)\n",
    "\n",
    "print(llama2_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_prompt = \"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "Your are a helpful assitant that extracts answers from a context passage given a question.\n",
    "<</SYS>>\n",
    "\n",
    "Context: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6½ sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5½ sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns.\n",
    "\n",
    "Question: How many points did the Panthers defense surrender?\n",
    "\n",
    "Answer: [/INST] 308 </s><s>[INST] Context: {{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Answer: [/INST]\"\"\"\n",
    "\n",
    "zero_shot_prompt = \"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "Your are a helpful assitant that extracts answers from a context passage given a question.\n",
    "<</SYS>>\n",
    "\n",
    "Context: {{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Answer: [/INST]\"\"\"\n",
    "\n",
    "default_prompt = \"\"\"\\\n",
    "Context: {{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "---\n",
    "- Prompting format only affects llama-chat model?\n",
    "- One-shot chat: 0.3 exact, 0.83 solution present\n",
    "- 0-shot chat: 0.0 exact, 0.85 solution present\n",
    "- dfault prompt chat: 0.0 exact, 0.86 solution present\n",
    "- 0-shot llama: 0.0 exact, 0.63 solution present\n",
    "-1-shot llama: 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.eval import EvalCollator\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset[\"validation\"].map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "\n",
    "collate_fn = EvalCollator(tokenizer, enc_tokenizer, mode = \"q\", context_enc = True, cover_labels = True, context_column=\"answers\")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size = 4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch, gen_batch, answers = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = loss_batch[\"decoder_input_ids\"][0]\n",
    "attn_mask = loss_batch[\"decoder_attention_mask\"][0]\n",
    "input_ids = input_ids[attn_mask == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = model.generate(**gen_batch, max_new_tokens=30, eos_token_id = tokenizer.eos_token_id, output_scores  = True, return_dict_in_generate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(gen.scores)[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_pickle(\"checkpoints/run_leafy-plasma-470/eval_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(gen.scores).argmax(dim= -1).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.scores[1].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(gen.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"eval_output.pkl\")\n",
    "\n",
    "#pd.read_csv(\"eval_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k:torch.tensor(v).unsqueeze(0) for k,v in df[\"gen_batch\"][0].items()}\n",
    "out_test = model.generate(**batch, max_new_tokens=30, eos_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_gpt2 import GPT2LMHeadModel, GPT2Config\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from torch import nn\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        enc_hidden_state = encoder(input_ids = batch[\"input_ids\"], \n",
    "                                   attention_mask = batch[\"attention_mask\"]).last_hidden_state\n",
    "        \n",
    "        out = decoder(input_ids = batch[\"decoder_input_ids\"], \n",
    "                     attention_mask = batch[\"decoder_attention_mask\"],\n",
    "                     encoder_hidden_states = enc_hidden_state, \n",
    "                     encoder_attention_mask = batch[\"attention_mask\"], \n",
    "                     labels = batch[\"labels\"])\n",
    "        return out\n",
    "\n",
    "def freeze_decoder(model):\n",
    "    for n,p in model.named_parameters():\n",
    "        if not \"cross\" in n:\n",
    "            p.requires_grad = False\n",
    "\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "config.add_cross_attention = True\n",
    "config.cross_attn_layer_idx = [11]\n",
    "\n",
    "decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\", config = config)\n",
    "encoder = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dec_tokenizer.pad_token = dec_tokenizer.eos_token\n",
    "dec_tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= dec_tokenizer.bos_token + \" $A \" + dec_tokenizer.eos_token,\n",
    "    special_tokens=[(dec_tokenizer.bos_token, dec_tokenizer.bos_token_id),(dec_tokenizer.eos_token, dec_tokenizer.eos_token_id)],\n",
    ")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "freeze_decoder(decoder)\n",
    "\n",
    "model  = EncoderDecoder(encoder = encoder,decoder = decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_length = 0\n",
    "input_ids = dec_tokenizer(\"hello this is me and my cat and we be doing a thing\", return_tensors = \"pt\")[\"input_ids\"]\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "def get_embeds(decoder, input_ids):\n",
    "    input_shape = input_ids.size()\n",
    "    position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=decoder.device)\n",
    "    position_ids = position_ids.unsqueeze(0)\n",
    "    inputs_embeds = decoder.transformer.wte(input_ids)\n",
    "    position_embeds = decoder.transformer.wpe(position_ids)\n",
    "    hidden_states = inputs_embeds + position_embeds\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = dec_tokenizer(\"The Space Needle is in the city of \", return_tensors = \"pt\")[\"input_ids\"]\n",
    "\n",
    "out =model.decoder(input_ids = input_ids[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer.decode(out.logits.squeeze()[-1].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.decoder.generate(input_ids = input_ids[:,:-1], max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer(\"The Space Needle is in the city of \", return_tensors = \"pt\")[\"input_ids\"]\n",
    "out = model.generate(input_ids = input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_path = \"checkpoints/gpt2_q_squad_run_cosmic-butterfly-490/checkpoint-5475/\"\n",
    "model_path = \"checkpoints/gpt2medium_q_squad_run_summer-cloud-510/checkpoint-5475/\"\n",
    "model_path = \"checkpoints/gpt2medium_run_good-puddle-513/checkpoint-5475/\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= tokenizer.bos_token + \" $A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.bos_token, tokenizer.bos_token_id),(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "\n",
    "from train_utils.eval import EvalCollator\n",
    "\n",
    "collate_fn = EvalCollator(tokenizer, enc_tokenizer= None, mode = \"q\", context_enc = False, cover_labels = False)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Calculate indices for the last 15 rows\n",
    "dataset_train = dataset[\"train\"]\n",
    "total_rows = len(dataset[\"train\"])\n",
    "last_15_indices = list(range(total_rows - 15, total_rows))\n",
    "\n",
    "# Select the last 15 rows, keeping it as a Hugging Face dataset\n",
    "last_15_rows = dataset_train.select(last_15_indices)\n",
    "\n",
    "loader = DataLoader(last_15_rows, batch_size = 10, collate_fn=collate_fn)\n",
    "batch_loss, batch_gen, answers = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils.loading_utils import load_encdec_model, load_batches_from_evaldf\n",
    "from transformers import AutoModel\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\", AutoModel, GPT2LMHeadModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/eval_output.pkl\")\n",
    "loss_batch, gen_batch, dec_loss_batch, dec_gen_batch = load_batches_from_evaldf(df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nethook import TraceDict\n",
    "import torch\n",
    "\n",
    "\n",
    "last_hidden_state_init = model.encoder(input_ids = loss_batch[\"input_ids\"], attention_mask = loss_batch[\"attention_mask\"]).last_hidden_state\n",
    "last_hidden_state = model.enc_to_dec_proj(last_hidden_state_init)\n",
    "\n",
    "\n",
    "layer_names = [n for n,p in model.named_modules() if \"cross\" not in n and \"internal_dropout\" not in n][2:]\n",
    "\n",
    "with TraceDict(model, layer_names) as ret:\n",
    "    out = model.decoder(input_ids = loss_batch[\"decoder_input_ids\"], \n",
    "                        attention_mask = loss_batch[\"decoder_attention_mask\"], \n",
    "                        encoder_hidden_states = last_hidden_state, \n",
    "                        encoder_attention_mask = loss_batch[\"attention_mask\"], \n",
    "                        labels = loss_batch[\"labels\"])\n",
    "    \n",
    "with TraceDict(model, layer_names) as ret_dec:\n",
    "    out_dec = model.decoder(input_ids = dec_loss_batch[\"input_ids\"],\n",
    "                            attention_mask = dec_loss_batch[\"attention_mask\"],\n",
    "                            labels = dec_loss_batch[\"labels\"])\n",
    "    \n",
    "ret_f = {k:v.output if isinstance(v.output, torch.Tensor) else v.output[0] for k,v in ret.items() if hasattr(v, \"output\")}\n",
    "ret_dec_f = {k:v.output if isinstance(v.output, torch.Tensor) else v.output[0] for k,v in ret_dec.items() if hasattr(v, \"output\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k1,v1), (k2,v2) in zip(ret_f.items(), ret_dec_f.items()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "enc_base = AutoModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_init = enc_base(loss_batch[\"input_ids\"], attention_mask = loss_batch[\"attention_mask\"]).last_hidden_state\n",
    "t = model.enc_to_dec_proj(t_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean tensor diff t_init {torch.tensor([(t_init[0][9] - t_init[0][i]).abs().mean() for i in range(30)]).mean()}\")\n",
    "print(f\"Mean tensor diff t {torch.tensor([(t[0][9] - t[0][i]).abs().mean() for i in range(30)]).mean()}\")\n",
    "print(f\"Mean tensor diff last_h_init {torch.tensor([(last_hidden_state_init[0][9] - last_hidden_state_init[0][i]).abs().mean() for i in range(30)]).mean()}\")\n",
    "print(f\"Mean tensor diff last_h {torch.tensor([(last_hidden_state[0][9] - last_hidden_state[0][i]).abs().mean() for i in range(30)]).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Example words\n",
    "words = ['king', 'queen', 'apple', 'oof', 'table']\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = [get_embedding(word, enc_base, enc_tokenizer).detach().squeeze().numpy() for word in words]\n",
    "embeddings2  = [get_embedding(word, model.encoder, enc_tokenizer).detach().squeeze().numpy() for word in words]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(cos_sim, annot=True, xticklabels=words, yticklabels=words)\n",
    "plt.title('Cosine Similarity between Word Embeddings')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Embedding Collapse in Enc model\n",
    "---\n",
    "- We see that the vector space of the finetuned encoder collapsed as the cosine sim is 1 of embeds and the mean abs diff is extremly low\n",
    "- The pca plot shows how the distribution collapsed onto a single principal component. Also consider the scale of the pca\n",
    "- We are not using tsne or umap as they scale the values for dim reduction thus hiding the true scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "enc_base = AutoModel.from_pretrained(\"roberta-base\")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "from eval_utils.loading_utils import load_encdec_model, load_batches_from_evaldf\n",
    "from transformers import AutoModel\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "checkpoint_path = \"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\"\n",
    "# Checkpoing with var type norm of cross attn\n",
    "checkpoint_path = \"checkpoints/run_divine-field-529/checkpoint-5475/\"\n",
    "\n",
    "model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(checkpoint_path, AutoModel, GPT2LMHeadModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"DeepMind Technologies Limited,[4] doing business as Google DeepMind, is a British-American artificial intelligence research laboratory which serves as a subsidiary of Google. Founded in the UK in 2010, it was acquired by Google in 2014,[5] The company is based in London, with research centres in Canada,[6] France,[7] Germany and the United States.\n",
    "\n",
    "Google DeepMind has created neural network models that learn how to play video games in a fashion similar to that of humans,[8] as well as Neural Turing machines (neural networks that can access external memory like a conventional Turing machine),[9] resulting in a computer that loosely resembles short-term memory in the human brain.[10][11]\n",
    "\n",
    "DeepMind made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, a world champion, in a five-game match, which was the subject of a documentary film.[12] A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning.[13] In 2020, DeepMind made significant advances in the problem of protein folding with AlphaFold.[14] In July 2022, it was announced that over 200 million predicted protein structures, representing virtually all known proteins, would be released on the AlphaFold database.[15][16] \"\"\"\n",
    "\n",
    "input_tokens = enc_tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils.eval_tools import get_similarities\n",
    "\n",
    "base_states = enc_base(**input_tokens).last_hidden_state.squeeze().detach()\n",
    "base_states_np = base_states.numpy()\n",
    "\n",
    "fine_states = model.encoder(**input_tokens).last_hidden_state.squeeze().detach()\n",
    "fine_states_np = fine_states.numpy()\n",
    "\n",
    "print(f\"Pretrained Model\")\n",
    "diff_matrix, cos_sim = get_similarities(base_states)\n",
    "print(f\"Avg abs dist: {diff_matrix.mean(dim=-1).mean()}\")\n",
    "print(f\"Avg abs cos dist: {cos_sim.mean(dim=-1).mean()}\")\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "print(f\"Fine-Tuned Model\")\n",
    "diff_matrix2, cos_sim2 = get_similarities(fine_states)\n",
    "print(f\"Avg abs dist: {diff_matrix2.mean(dim=-1).mean()}\")\n",
    "print(f\"Avg abs cos dist: {cos_sim2.mean(dim=-1).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils.eval_tools import plot_tsne, plot_pca\n",
    "\n",
    "#plot_tsne(base_states_np)\n",
    "#plot_tsne(fine_states_np)\n",
    "\n",
    "plot_pca(base_states_np)\n",
    "plot_pca(fine_states_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating decoder logits\n",
    "---\n",
    "- What we observe is that the signal of the encoder even in a randomly initalized state is really weak\n",
    "- The enc signal has almost no effect on the loss and almost no effect on the logits\n",
    "- Implemented different positions of cross-attn: almost no effect: There is almost no difference in loss between cross attn and no cross attn\n",
    "- Loss explosion is caused by applying layer norm on residual when applying cross attn\n",
    "\n",
    "- As an analysis we perform jaccard dist and rob sim between the rankings of the top 10 logits\n",
    "- We also extract the logprobs of the first ref token to see how it changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from eval_utils.loading_utils import load_encdec_model, load_batches_from_evaldf\n",
    "from transformers import AutoModel\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "\n",
    "enc_base = AutoModel.from_pretrained(\"roberta-base\")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "#config = AutoConfig.from_pretrained(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\").decoder\n",
    "#dec_base = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config = config)\n",
    "\n",
    "model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\", AutoModel, GPT2LMHeadModel)\n",
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from eval_utils.loading_utils import load_batches_from_evaldf\n",
    "\n",
    "df = pd.read_pickle(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/eval_output.pkl\")\n",
    "\n",
    "loss_batch, gen_batch, dec_loss_batch, dec_gen_batch = load_batches_from_evaldf(df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "from modeling_gpt2 import GPT2Attention\n",
    "from torch import nn\n",
    "\n",
    "dec_conf = model.decoder.config\n",
    "\n",
    "# Default enc_dec forward\n",
    "output_encdec = model.generate(**gen_batch, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id, return_dict_in_generate = True, output_scores = True)\n",
    "\n",
    "# Encdec with random cross attention\n",
    "model.decoder.transformer.h[7].crossattention = GPT2Attention(dec_conf, layer_idx=7, is_cross_attention=True)\n",
    "model.decoder.transformer.h[8].crossattention = GPT2Attention(dec_conf, layer_idx=8, is_cross_attention=True)\n",
    "model.decoder.transformer.h[9].crossattention = GPT2Attention(dec_conf, layer_idx=9, is_cross_attention=True)\n",
    "model.decoder.transformer.h[10].crossattention = GPT2Attention(dec_conf, layer_idx=10, is_cross_attention=True)\n",
    "model.decoder.transformer.h[7].ln_cross_attn =nn.LayerNorm(1024, eps=dec_conf.layer_norm_epsilon)\n",
    "model.decoder.transformer.h[8].ln_cross_attn =nn.LayerNorm(1024, eps=dec_conf.layer_norm_epsilon)\n",
    "model.decoder.transformer.h[9].ln_cross_attn =nn.LayerNorm(1024, eps=dec_conf.layer_norm_epsilon)\n",
    "model.decoder.transformer.h[10].ln_cross_attn =nn.LayerNorm(1024, eps=dec_conf.layer_norm_epsilon)\n",
    "output_encdec_rand = model.generate(**gen_batch, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id, return_dict_in_generate = True, output_scores = True)\n",
    "\n",
    "# With random cross attention and random enc\n",
    "model.encoder = enc_base\n",
    "output_encdec_base = model.generate(**gen_batch, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id, return_dict_in_generate = True, output_scores = True)\n",
    "\n",
    "# Decoder only\n",
    "output_dec = model.decoder.generate(**dec_gen_batch, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id, return_dict_in_generate = True, output_scores = True)\n",
    "\n",
    "\n",
    "# Transpose to shape (batch_size, num_tokens, vocab_size)\n",
    "softmax_scores_encdec = F.softmax(torch.stack(output_encdec.scores),dim = -1).transpose(0,1)\n",
    "softmax_scores_dec = F.softmax(torch.stack(output_dec.scores),dim = -1).transpose(0,1)\n",
    "softmax_scores_encdec_base = F.softmax(torch.stack(output_encdec_base.scores),dim = -1).transpose(0,1)\n",
    "softmax_scores_encdec_rand = F.softmax(torch.stack(output_encdec_rand.scores),dim = -1).transpose(0,1)\n",
    "\n",
    "log_prob, index_encdec_base = softmax_scores_encdec_base.topk(10)\n",
    "log_prob, index_encdec_rand = softmax_scores_encdec_rand.topk(10)\n",
    "log_prob, index_encdec = softmax_scores_encdec.topk(10)\n",
    "log_prob, index_dec = softmax_scores_dec.topk(10)\n",
    "\n",
    "\n",
    "min_token = min(len(index_encdec_base), len(index_encdec), len(index_dec), len(index_encdec_rand))\n",
    "index_encdec_base = index_encdec_base[:,:min_token,:]\n",
    "index_encdec_rand = index_encdec_rand[:,:min_token,:]\n",
    "index_encdec = index_encdec[:,:min_token,:]\n",
    "index_dec = index_dec[:,:min_token,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from eval_utils.rbo import rbo\n",
    "\n",
    "def get_ref_logprobs(softmax_scores, first_ref_token_idx):\n",
    "    \"\"\"Inputs:\n",
    "    softmax_scores: Tensor of shape (batch_size, num_tokens, vocab_size) !this is a transpose of the original tensor!\n",
    "    first_ref_token_idx: List of shape (batch_size) containing the index of the first reference token in each batch\n",
    "    \"\"\"\n",
    "\n",
    "    ref_logprobs = []\n",
    "    for log_probs, ref_id in zip(softmax_scores, first_ref_token_idx):\n",
    "        ref_logprobs.append(log_probs[0][ref_id].item())\n",
    "    return ref_logprobs\n",
    "\n",
    "def calculate_logit_stats(tensor1, tensor2):\n",
    "    # Calculate jaccard similarity for top k logit indices\n",
    "    jaccard_similarities = np.zeros((tensor1.shape[0], tensor1.shape[1]))\n",
    "    rbo_data = np.zeros((tensor1.shape[0], tensor1.shape[1]))\n",
    "\n",
    "    # Iterate over the tensor elements\n",
    "    for i in range(tensor1.shape[0]):\n",
    "        for j in range(tensor1.shape[1]):\n",
    "            # Convert tensor slices to sets\n",
    "            set1 = set(tensor1[i, j].numpy())\n",
    "            set2 = set(tensor2[i, j].numpy())\n",
    "\n",
    "            # RBO\n",
    "            rbo_res = rbo(list(tensor1[i][j].numpy()), list(tensor2[i][j].numpy()), p = 0.9)\n",
    "            rbo_data[i, j] = rbo_res.ext\n",
    "\n",
    "            # Calculate Jaccard similarity\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            jaccard_sim = intersection / union if union != 0 else 0\n",
    "\n",
    "            # Store the similarity\n",
    "            jaccard_similarities[i, j] = jaccard_sim\n",
    "\n",
    "    return jaccard_similarities, rbo_data\n",
    "\n",
    "jaccard, rbo = calculate_logit_stats(index_enc_dec2, index_dec2)\n",
    "#jaccard, rbo = calculate_logit_stats(index_encdec, index_dec)\n",
    "\n",
    "print(jaccard.mean())\n",
    "print(rbo.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"checkpoints/eval_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "enc_dec_logits = torch.tensor(df.gen_logits[0])\n",
    "dec_logits = torch.tensor(df.decoder_gen_logits[0][:len(enc_dec_logits),:])\n",
    "enc_dec_logits1 = torch.tensor(df.gen_logits[1])\n",
    "dec_logits1 = torch.tensor(df.decoder_gen_logits[1][:len(enc_dec_logits),:])\n",
    "enc_dec_logits2 = torch.tensor(df.gen_logits[2])\n",
    "dec_logits2 = torch.tensor(df.decoder_gen_logits[2][:len(enc_dec_logits),:])\n",
    "\n",
    "enc_dec_logits = torch.stack([enc_dec_logits, enc_dec_logits1, enc_dec_logits2])\n",
    "dec_logits = torch.stack([dec_logits, dec_logits1, dec_logits2])\n",
    "\n",
    "\n",
    "log_prob, index_enc_dec2 = enc_dec_logits.topk(10)\n",
    "log_prob, index_dec2 = dec_logits.topk(10)\n",
    "\n",
    "jaccard, rob = calculate_logit_stats(index_enc_dec2, index_dec2)\n",
    "print(jaccard.mean())\n",
    "pritn(rob.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_enc_dec.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_logit_stats(index_encdec, index_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbo.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = loss_batch[\"labels\"]\n",
    "first_ref_token_idx = [\n",
    "    [value.item() for value in row if value not in (-100, 25)][0] for row in input_tensor\n",
    "]\n",
    "ref_logprob_encdec = get_ref_logprobs(softmax_scores_encdec, first_ref_token_idx)\n",
    "ref_logprob_dec = get_ref_logprobs(softmax_scores_dec, first_ref_token_idx)\n",
    "ref_logprob_encdec_base = get_ref_logprobs(softmax_scores_encdec_base, first_ref_token_idx)\n",
    "ref_logprob_encdec_rand = get_ref_logprobs(softmax_scores_encdec_rand, first_ref_token_idx)\n",
    "print(\"Logprob of first ref token:\")\n",
    "print(ref_logprob_encdec)\n",
    "print(ref_logprob_dec)\n",
    "print(ref_logprob_encdec_base)\n",
    "print(ref_logprob_encdec_rand)\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing how the encoder signal flows in the model\n",
    "---\n",
    "- Created a plot to show how different acitvations are for encdec and dec-only forwarf pass\n",
    "- We see that the differences are there but they are not that pronounced \n",
    "- This shows that the signals are indeed affecting the model and are even becoming more prominent towards the end but overall the magnitude of the changes is too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from eval_utils.loading_utils import load_encdec_model, load_batches_from_evaldf\n",
    "from transformers import AutoModel\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "\n",
    "enc_base = AutoModel.from_pretrained(\"roberta-base\")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "#config = AutoConfig.from_pretrained(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\").decoder\n",
    "#dec_base = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config = config)\n",
    "\n",
    "checkpoint_path = \"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\"\n",
    "#checkpoint_path = \"checkpoints/run_divine-field-529/checkpoint-5475\"\n",
    "\n",
    "model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(checkpoint_path, AutoModel, GPT2LMHeadModel)\n",
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from eval_utils.loading_utils import load_batches_from_evaldf\n",
    "\n",
    "df = pd.read_pickle(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/eval_output.pkl\")\n",
    "\n",
    "loss_batch, gen_batch, dec_loss_batch, dec_gen_batch = load_batches_from_evaldf(df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nethook import TraceDict\n",
    "import torch\n",
    "\n",
    "layer_names = [\"decoder.\"+n for n,m in model.decoder.named_modules() if len(n.split(\".\")) == 3]\n",
    "\n",
    "with TraceDict(model, layer_names) as ret:\n",
    "    _ = model(**gen_batch)\n",
    "outputs = {k:v.output if isinstance(v.output, torch.Tensor) else v.output[0] for k,v in ret.items() if hasattr(v, \"output\")}\n",
    "\n",
    "\n",
    "with TraceDict(model, layer_names) as ret_dec:\n",
    "    _ = model.decoder(**dec_gen_batch)\n",
    "outputs_dec = {k:v.output if isinstance(v.output, torch.Tensor) else v.output[0] for k,v in ret_dec.items() if hasattr(v, \"output\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "outputs[\"decoder.lm_head\"] = F.softmax(outputs[\"decoder.lm_head\"], dim = -1)\n",
    "outputs_dec[\"decoder.lm_head\"] = F.softmax(outputs_dec[\"decoder.lm_head\"], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The abs mean diff between dec only activations and encdec forward pass\n",
    "output_diff= {k: (v-outputs[k]).abs().mean(dim = -1)  for k,v in outputs_dec.items()}\n",
    "\n",
    "# The abs mean diff for each activation in the model\n",
    "differences = [diff[0].detach() for diff in output_diff.values()]\n",
    "differences = torch.stack(differences).numpy().T\n",
    "\n",
    "# The text for each token in the sequence\n",
    "tokens = dec_tokenizer.batch_decode(dec_gen_batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.5, 2), dpi=200)\n",
    "h = ax.pcolor(\n",
    "    differences,\n",
    "    cmap={None: \"Purples\", \"None\": \"Purples\", \"mlp\": \"Greens\", \"attn\": \"Reds\"}[\n",
    "        None\n",
    "    ],\n",
    "    vmin=0.0,\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks([0.5 + i for i in range(len(differences))])\n",
    "ax.set_xticks([0.5 + i for i in range(0, differences.shape[1],2)])\n",
    "ax.set_xticklabels(list(range(0, differences.shape[1]-1, 2)), fontsize = 7)\n",
    "#ax.set_xticklabels(list(range(0, differences.shape[1]-1, 2))+[\"lm_head\"], fontsize = 7)\n",
    "#if (xticklabels:=ax.get_xticklabels()): xticklabels[-1].set_rotation(45)\n",
    "ax.set_yticklabels(tokens, fontsize = 7)\n",
    "ax.set_xlabel(f\"Layer id\", fontsize = 7)\n",
    "ax.set_title(\"Difference in layer activations\", fontsize = 10)\n",
    "cb = plt.colorbar(h)\n",
    "cb.ax.tick_params(labelsize=7) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nethook import TraceDict\n",
    "import torch\n",
    "\n",
    "layer_names = [\"decoder.\"+n for n,m in model.decoder.named_modules() if len(n.split(\".\")) == 4] + [\"decoder.lm_head\"]\n",
    "\n",
    "with TraceDict(model, layer_names) as ret:\n",
    "    _ = model(**gen_batch)\n",
    "outputs = {k:v.output if isinstance(v.output, torch.Tensor) else v.output[0] for k,v in ret.items() if hasattr(v, \"output\")}\n",
    "\n",
    "\n",
    "with TraceDict(model, layer_names) as ret_dec:\n",
    "    _ = model.decoder(**dec_gen_batch)\n",
    "outputs_dec = {k:v.output if isinstance(v.output, torch.Tensor) else v.output[0] for k,v in ret_dec.items() if hasattr(v, \"output\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"checkpoints/eval_output2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gen_batch\"][0][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.f1!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dec[\"decoder.lm_head\"][:,-1,:].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_diff= {k: (v-outputs[k]).abs().mean(dim = -1)  for k,v in outputs_dec.items()}\n",
    "for k,v in output_diff.items():\n",
    "    print(f\"{k}: {v.mean(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Analyzing Differences in loss \n",
    " - Even if we randomly initialize the cross attention modules and use a default pretrained encoder the loss is barely affected\n",
    " - This is true no matter where the crossattn block is positioned i.e. after attention, after mlp, after everthing (final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from eval_utils.loading_utils import load_encdec_model, load_batches_from_evaldf\n",
    "from transformers import AutoModel\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "\n",
    "enc_base = AutoModel.from_pretrained(\"roberta-base\")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "#config = AutoConfig.from_pretrained(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\").decoder\n",
    "#dec_base = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config = config)\n",
    "\n",
    "#model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(\"checkpoints/run_divine-field-529/checkpoint-5475\", AutoModel, GPT2LMHeadModel)\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "dec_tokenizer.pad_token = dec_tokenizer.eos_token\n",
    "dec_tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= dec_tokenizer.bos_token + \" $A \" + dec_tokenizer.eos_token,\n",
    "    special_tokens=[(dec_tokenizer.bos_token, dec_tokenizer.bos_token_id),(dec_tokenizer.eos_token, dec_tokenizer.eos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from eval_utils.loading_utils import load_batches_from_evaldf\n",
    "\n",
    "df = pd.read_pickle(\"checkpoints/run_divine-field-529/eval_output.pkl\")\n",
    "\n",
    "loss_batch, gen_batch, dec_loss_batch, dec_gen_batch = load_batches_from_evaldf(df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.utils import prepare_dataset, prompt_qa, prompt_qc_enc\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from train_utils.encoder import PrefixEncoder\n",
    "\n",
    "enc_model, enc_tokenizer = PrefixEncoder.from_sentenc_checkpoint(\"checkpoints/2725_prefix10/\")\n",
    "\n",
    "dataset = load_from_disk(\"squad_with_answer_sentence\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "dataset = dataset.map(lambda x: {k:v.strip() for k,v in x.items()})\n",
    "\n",
    "df_qca = dataset.map(prepare_dataset, \n",
    "                     fn_kwargs={\"prompt\": prompt_qa, \n",
    "                                \"tokenizer\": dec_tokenizer, \n",
    "                                \"create_labels\" : False, \n",
    "                                \"enc_tokenizer\": enc_tokenizer, \n",
    "                                \"context_enc\": True, \n",
    "                                \"context_column\": \"answer_sentence\",\n",
    "                                \"answer_column\": \"answers\",\n",
    "                                \"enc_prompt\": prompt_qc_enc,\n",
    "                                \"num_prefix_token\": 5}, \n",
    "                     batched=True, \n",
    "                     remove_columns=dataset[\"train\"].column_names)\n",
    "from train_utils.utils import CustomCollator\n",
    "from torch.utils.data import DataLoader\n",
    "collate_fn = CustomCollator(dec_tokenizer, enc_tokenizer = enc_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from train_utils.eval import EvalCollator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset = dataset.map(lambda x: {\"answers\": x[\"answers\"][\"text\"][0]})\n",
    "loader = DataLoader(dataset[\"train\"], batch_size = 3, collate_fn = EvalCollator(tokenizer,\n",
    "                                                                                enc_tokenizer, \n",
    "                                                                                mode = \"q\", \n",
    "                                                                                context_enc = True, \n",
    "                                                                                cover_labels=True, \n",
    "                                                                                context_column = \"qc\",\n",
    "                                                                                answer_column = \"answers\",\n",
    "                                                                                num_prefix_token = 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch, gen_batch, answers = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.num_prefix_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer.batch_decode(df_qca[\"validation\"].to_pandas().iloc[0].input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(df_qca[\"train\"], batch_size = 4, collate_fn=collate_fn)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "for i , batch in enumerate(loader):\n",
    "    print(model(**batch).loss)\n",
    "    if i == 10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    loss_batch, gen_batch, dec_loss_batch, dec_gen_batch = load_batches_from_evaldf(df, i)\n",
    "    print(model(**loss_batch).loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "out = model(**loss_batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "out = model(**loss_batch)\n",
    "out_dec = model.decoder(**dec_loss_batch)\n",
    "print(out.loss)\n",
    "print(out_dec.loss)\n",
    "dec_tokenizer.batch_decode(out.logits.argmax(dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_v = 0.1\n",
    "\n",
    "model.encoder.embeddings.dropout.p = dropout_v\n",
    "for i in range(12):\n",
    "    model.encoder.encoder.layer[i].attention.self.dropout.p = dropout_v\n",
    "    model.encoder.encoder.layer[i].attention.output.dropout.p = dropout_v\n",
    "    model.encoder.encoder.layer[i].output.dropout.p = dropout_v\n",
    "\n",
    "for i in range(len(model.decoder.transformer.h)):\n",
    "    model.decoder.transformer.h[7].crossattention.attn_dropout.p =  dropout_v\n",
    "    model.decoder.transformer.h[8].crossattention.resid_dropout.p =  dropout_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model(**loss_batch)\n",
    "out_dec = model.decoder(**dec_loss_batch)\n",
    "print(out.loss)\n",
    "print(out_dec.loss)\n",
    "dec_tokenizer.batch_decode(out.logits.argmax(dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model.generate(**gen_batch, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id)\n",
    "dec_tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "out = model.generate(**gen_batch, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id)\n",
    "dec_tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "layer_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "config.know_layer = layer_ids\n",
    "config.know_type = \"crossattn\"\n",
    "config.hidden_dropout = 0.0\n",
    "config.know_pos = \"final\"\n",
    "config.know_norm = \"var\"\n",
    "dec_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config = config)\n",
    "\n",
    "enc_base = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from eval_utils.loading_utils import load_batches_from_evaldf\n",
    "\n",
    "df = pd.read_pickle(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/eval_output.pkl\")\n",
    "\n",
    "loss_batch, gen_batch, dec_loss_batch, dec_gen_batch = load_batches_from_evaldf(df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in layer_ids:\n",
    "    dec_model.transformer.h[i].cross_attn_pos = \"mlp\"\n",
    "    dec_model.transformer.h[i].cross_attn_norm = \"var\"\n",
    "\n",
    "hidden_states = enc_base(input_ids = loss_batch[\"input_ids\"], attention_mask = loss_batch[\"attention_mask\"]).last_hidden_state\n",
    "hidden_states = hidden_states * 1\n",
    "\n",
    "loss_encdec = dec_model(input_ids = loss_batch[\"decoder_input_ids\"],\n",
    "          attention_mask = loss_batch[\"decoder_attention_mask\"],\n",
    "          encoder_hidden_states = hidden_states,\n",
    "          encoder_attention_mask = loss_batch[\"attention_mask\"],\n",
    "          labels = loss_batch[\"labels\"]).loss\n",
    "\n",
    "out = dec_model(**dec_loss_batch)\n",
    "loss_dec = out.loss\n",
    "\n",
    "print(loss_encdec)\n",
    "print(loss_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dec_model.generate(**dec_gen_batch, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id, return_dict_in_generate = True, output_scores = True)\n",
    "dec_tokenizer.batch_decode(out.sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dec_model.generate(input_ids = dec_gen_batch[\"input_ids\"], attention_mask = dec_gen_batch[\"attention_mask\"], encoder_hidden_states = hidden_states, encoder_attention_mask = gen_batch[\"attention_mask\"], max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id)\n",
    "dec_tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "checkpoint_path = \"checkpoints/run_divine-field-529/checkpoint-5475/\"\n",
    "conf = AutoConfig.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_pickle(\"checkpoints/eval_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.exact_match != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_to_dec_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_parameters(model):\n",
    "    encoder_nodecay = [p for n,p in model.encoder.named_parameters() if any([x in n for x in [\"bias\", \"LayerNorm\"]])]\n",
    "    encoder_decay = [p for n,p in model.encoder.named_parameters() if not any([x in n for x in [\"bias\", \"LayerNorm\"]])]\n",
    "\n",
    "    decoder_cross_nodecay = [p for n,p in model.decoder.named_parameters() if any([x in n for x in [\"cross\",\"attn_gate\"]]) and any([x in n for x in [\"bias\", \"ln\"]])]\n",
    "    decoder_cross_decay = [p for n,p in model.decoder.named_parameters() if any([x in n for x in [\"cross\",\"attn_gate\"]]) and not any([x in n for x in [\"bias\", \"ln\"]])]\n",
    "\n",
    "    decoder_backbone_nodecay = [p for n,p in model.decoder.named_parameters() if not any([x in n for x in [\"cross\",\"attn_gate\"]]) and any([x in n for x in [\"bias\", \"ln\"]])]\n",
    "    decoder_backbone_decay = [p for n,p in model.decoder.named_parameters() if not any([x in n for x in [\"cross\",\"attn_gate\"]]) and not any([x in n for x in [\"bias\", \"ln\"]])]\n",
    "\n",
    "    if hasattr(model, \"enc_to_dec_proj\"):\n",
    "        decoder_cross_nodecay += [model.enc_to_dec_proj.bias]\n",
    "        decoder_cross_decay += [model.enc_to_dec_proj.weight]\n",
    "    return {\"decoder_backbone_decay\": decoder_backbone_decay,\n",
    "            \"decoder_backbone_nodecay\": decoder_backbone_nodecay,\n",
    "            \"decoder_cross_decay\": decoder_cross_decay,\n",
    "            \"decoder_cross_nodecay\": decoder_cross_nodecay,\n",
    "            \"encoder_decay\": encoder_decay,\n",
    "            \"encoder_nodecay\": encoder_nodecay}\n",
    "\n",
    "\n",
    "def get_group_params(train_type:str, param_dict):    \n",
    "    params_decay = []\n",
    "    params_nodecay = []\n",
    "    if \"enc\" in train_type or train_type == \"full\":\n",
    "        params_decay+= param_dict.pop(\"encoder_decay\") if \"encoder_decay\" in param_dict else []\n",
    "        params_nodecay+= param_dict.pop(\"encoder_nodecay\") if \"encoder_nodecay\" in param_dict else []\n",
    "    if \"cross\" in train_type or train_type == \"full\":\n",
    "        params_decay += param_dict.pop(\"decoder_cross_decay\") if \"decoder_cross_decay\" in param_dict else []\n",
    "        params_nodecay += param_dict.pop(\"decoder_cross_nodecay\") if \"decoder_cross_nodecay\" in param_dict else []\n",
    "    if \"dec\" in train_type or train_type == \"full\":\n",
    "        params_decay += param_dict.pop(\"decoder_backbone_decay\") if \"decoder_backbone_decay\" in param_dict else []\n",
    "        params_nodecay += param_dict.pop(\"decoder_backbone_nodecay\") if \"decoder_backbone_nodecay\" in param_dict else []\n",
    "    return params_decay, params_nodecay\n",
    "\n",
    "def get_g1_g2(model, train_type1, train_type2):\n",
    "    all_param_groups = prepare_model_parameters(model)\n",
    "    g1_decay_params, g1_nodecay_params = get_group_params(train_type1, all_param_groups)\n",
    "    g2_decay_params, g2_nodecay_params = get_group_params(train_type2, all_param_groups)\n",
    "    \n",
    "    return g1_decay_params, g1_nodecay_params, g2_decay_params, g2_nodecay_params\n",
    "\n",
    "\n",
    "g1_decay_params, g1_nodecay_params, g2_decay_params, g2_nodecay_params = get_g1_g2(model,\"full\", \"full\")\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from optim_scheduler import DifferentialAlignmentSchedulerWithZeroPeriodLRFixed\n",
    "\n",
    "\n",
    "\n",
    "def get_optimizer_and_scheduler(optim_args,g1_decay_params, g1_nodecay_params, g2_decay_params, g2_nodecay_params):\n",
    "    dummy_lr = 1e-9\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": g1_decay_params,\n",
    "            \"weight_decay\": optim_args.weight_decay,\n",
    "            'lr': dummy_lr\n",
    "        },\n",
    "        {\n",
    "            \"params\": g1_nodecay_params,\n",
    "            \"weight_decay\": 0.0,\n",
    "            'lr': dummy_lr\n",
    "        },\n",
    "        {\n",
    "            \"params\": g2_decay_params,\n",
    "            \"weight_decay\": optim_args.weight_decay,\n",
    "            'lr': dummy_lr\n",
    "        },\n",
    "        {\n",
    "            \"params\": g2_nodecay_params,\n",
    "            \"weight_decay\": 0.0,\n",
    "            'lr': dummy_lr\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "    scheduler = DifferentialAlignmentSchedulerWithZeroPeriodLRFixed(optimizer, \n",
    "                                                                    warmup_steps_g1= optim_args.warmup_steps_g1, \n",
    "                                                                    warmup_steps_g2= optim_args.warmup_steps_g2, \n",
    "                                                                    lr_g1= optim_args.lr_g1,\n",
    "                                                                    total_steps=optim_args.total_steps, \n",
    "                                                                    zero_period_steps = optim_args.zero_period_steps, \n",
    "                                                                    zero_period_lr = optim_args.zero_period_lr,\n",
    "                                                                    decay = optim_args.decay_type)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"checkpoints/eval_output_548.pkl\")\n",
    "\n",
    "print(df.exact_match.mean())\n",
    "print(df.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"crossenccrossenc\": (0.127,0.199), \"fullfull\": (0.127,0.22), \"crosscross\": (0.142,0.236), \"crossfull\": (0.127,0.2157), \"crosscrossenc\": (0.152, 0.25)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "optim_args = SimpleNamespace(weight_decay = 0.01, \n",
    "                            warmup_steps_g1= 500, \n",
    "                            warmup_steps_g2= 1000, \n",
    "                            zero_period_steps= 5475, \n",
    "                            total_steps= 10950, \n",
    "                            lr_g1= 1e-4, \n",
    "                            zero_period_lr= 0,\n",
    "                            decay_type = \"linear\")\n",
    "\n",
    "\n",
    "get_optimizer_and_scheduler(optim_args, g1_decay_params, g1_nodecay_params, g2_decay_params, g2_nodecay_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from eval_utils.loading_utils import load_encdec_model, load_batches_from_evaldf\n",
    "from transformers import AutoModel\n",
    "from modeling_gpt2 import GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "\n",
    "#enc_base = AutoModel.from_pretrained(\"roberta-base\")\n",
    "#enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "#config = AutoConfig.from_pretrained(\"checkpoints/gpt2medium-q-enc-squad_run_rich-night-520/checkpoint-5475\").decoder\n",
    "#dec_base = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", config = config)\n",
    "\n",
    "model, enc_tokenizer, dec_tokenizer, train_conf = load_encdec_model(\"checkpoints/run561_checkpoint-10950/\", AutoModel, GPT2LMHeadModel)\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "dec_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "dec_tokenizer.pad_token = dec_tokenizer.eos_token\n",
    "dec_tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single= dec_tokenizer.bos_token + \" $A \" + dec_tokenizer.eos_token,\n",
    "    special_tokens=[(dec_tokenizer.bos_token, dec_tokenizer.bos_token_id),(dec_tokenizer.eos_token, dec_tokenizer.eos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils.eval import evaluate\n",
    "\n",
    "output = evaluate(model, dec_tokenizer, enc_tokenizer, \"squad_with_answer_sentence\", \"q\", True, False, 1, \"answer_sentence\", \"answers\", False, max_batches=200, save_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer.batch_decode(output.gen_batch[0][\"decoder_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input = {k:torch.tensor(v).unsqueeze(0) for k,v in input.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(**input, max_new_tokens=30, eos_token_id = dec_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu.compute(predictions =[\"a \"], references=[\"abc\"], )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
