{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict \n",
    "import transformers\n",
    "\n",
    "# Adjusted from: https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py#L65\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "    cls_update: bool = False,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Resizes token embeds, and intializes new embeds with mean of vocab\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        if cls_update and tokenizer.cls_token_id is not None:\n",
    "            # For the prefix encoder set CLS embed for new prefix token\n",
    "            cls_embed = input_embeddings[tokenizer.cls_token_id]\n",
    "            input_embeddings[-num_new_tokens:] = cls_embed\n",
    "        else:\n",
    "            input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "            input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        # Encoder models may not have output embeds\n",
    "        if (output_embeds:=model.get_output_embeddings()):\n",
    "            output_embeddings = output_embeds.weight.data\n",
    "            output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "            output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_dataset(df_pd):\n",
    "    unique_context = df_pd.context.unique()\n",
    "    context_id_lookup = {context:i for i, context in enumerate(unique_context)}\n",
    "    df_pd[\"context_id\"] = df_pd.context.apply(lambda x: context_id_lookup[x])\n",
    "    df_pd[\"answers\"] = df_pd.apply(lambda x: x[\"answers\"][\"text\"][0], axis = 1)\n",
    "    context_answer_lookups = {context_id: context_group.answers.values for context_id, context_group in df_pd.groupby(\"context_id\")}\n",
    "    df_pd[\"false_answers\"] = df_pd.apply(lambda x: [answer for answer in context_answer_lookups[x[\"context_id\"]] if answer != x.answers], axis = 1)\n",
    "    df_pd = df_pd.drop([\"id\", \"title\", \"context_id\"], axis = 1)\n",
    "    return df_pd\n",
    "\n",
    "def create_classification_records(df, context_column = \"context\"):\n",
    "    df = prepare_dataset(df)\n",
    "    all_records = []\n",
    "    for i, row in df.iterrows():\n",
    "        false_records = [{\"context\":row[context_column], \"question\": row[\"question\"], \"answers\": false_answer, \"label\": 0} for false_answer in row[\"false_answers\"][:1]]\n",
    "        correct_record = {\"context\":row[context_column], \"question\": row[\"question\"], \"answers\": row[\"answers\"], \"label\": 1}\n",
    "        all_records.extend(false_records)\n",
    "        all_records.append(correct_record)\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, enc_model, num_prefix_tokens, num_labels=2):\n",
    "        super().__init__()\n",
    "        hidden_dim = enc_model.config.hidden_size\n",
    "        self.enc_model = enc_model\n",
    "        self.fusion_layer = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.classif_layer = nn.Linear(hidden_dim*num_prefix_tokens, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.num_labels = num_labels\n",
    "        self.num_prefix_tokens = num_prefix_tokens\n",
    "    \n",
    "    def forward(self, qc_input_ids, qc_attention_mask, a_input_ids, a_attention_mask, labels= None):\n",
    "        seq1_states = self.enc_model(input_ids = qc_input_ids, attention_mask = qc_attention_mask).last_hidden_state[:,:self.num_prefix_tokens,:]\n",
    "        seq2_states = self.enc_model(input_ids = a_input_ids, attention_mask = a_attention_mask).last_hidden_state[:,:self.num_prefix_tokens,:]\n",
    "\n",
    "        concat_seq = torch.cat((seq1_states, seq2_states), dim=-1)\n",
    "        concat_seq = self.fusion_layer(concat_seq)\n",
    "        concat_seq = nn.functional.relu(concat_seq)\n",
    "        concat_seq = concat_seq.reshape(len(qc_input_ids), -1)\n",
    "        concat_seq = self.dropout(concat_seq)\n",
    "        logits = self.classif_layer(concat_seq)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "    \n",
    "class ClassifierCollator:\n",
    "    def __init__(self, tokenizer, prompt_qc, prompt_a):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_qc = prompt_qc\n",
    "        self.prompt_a = prompt_a\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        qc_tokens = self.tokenizer([self.prompt_qc.format(**sample) for sample in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        a_tokens = self.tokenizer([self.prompt_a.format(**sample) for sample in batch], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        labels = torch.tensor([sample[\"label\"] for sample in batch])\n",
    "        return {\"qc_input_ids\": qc_tokens[\"input_ids\"], \n",
    "                \"qc_attention_mask\": qc_tokens[\"attention_mask\"], \n",
    "                \"a_input_ids\": a_tokens[\"input_ids\"], \n",
    "                \"a_attention_mask\": a_tokens[\"attention_mask\"],\n",
    "                \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "num_prefix_tokens = 3\n",
    "special_token_dict = {\"additional_special_tokens\": [f\"<PREFIX{i}>\" for i in range(num_prefix_tokens-1)]}\n",
    "\n",
    "enc_model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(special_token_dict, enc_tokenizer, enc_model, cls_update=True)\n",
    "model = CustomClassifier(enc_model, num_prefix_tokens=num_prefix_tokens, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"squad_with_answer_sentence\")\n",
    "\n",
    "df_pd_train = dataset[\"train\"].to_pandas()\n",
    "df_pd_validation = dataset[\"validation\"].to_pandas()\n",
    "\n",
    "df_train = create_classification_records(df_pd_train)\n",
    "df_val = create_classification_records(df_pd_validation)\n",
    "\n",
    "df_hf = DatasetDict({\"train\": Dataset.from_pandas(df_train), \"validation\": Dataset.from_pandas(df_val)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174794</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174795</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174796</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174797</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174798</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174799 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  context  \\\n",
       "0       Architecturally, the school has a Catholic cha...   \n",
       "1       Architecturally, the school has a Catholic cha...   \n",
       "2       Architecturally, the school has a Catholic cha...   \n",
       "3       Architecturally, the school has a Catholic cha...   \n",
       "4       Architecturally, the school has a Catholic cha...   \n",
       "...                                                   ...   \n",
       "174794  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "174795  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "174796  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "174797  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "174798  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                                 question  \\\n",
       "0       To whom did the Virgin Mary allegedly appear i...   \n",
       "1       To whom did the Virgin Mary allegedly appear i...   \n",
       "2       What is in front of the Notre Dame Main Building?   \n",
       "3       What is in front of the Notre Dame Main Building?   \n",
       "4       The Basilica of the Sacred heart at Notre Dame...   \n",
       "...                                                   ...   \n",
       "174794  With what Belorussian city does Kathmandu have...   \n",
       "174795  In what year did Kathmandu create its initial ...   \n",
       "174796  In what year did Kathmandu create its initial ...   \n",
       "174797                      What is KMC an initialism of?   \n",
       "174798                      What is KMC an initialism of?   \n",
       "\n",
       "                            answers  label  \n",
       "0         a copper statue of Christ      0  \n",
       "1        Saint Bernadette Soubirous      1  \n",
       "2        Saint Bernadette Soubirous      0  \n",
       "3         a copper statue of Christ      1  \n",
       "4        Saint Bernadette Soubirous      0  \n",
       "...                             ...    ...  \n",
       "174794                        Minsk      1  \n",
       "174795                       Oregon      0  \n",
       "174796                         1975      1  \n",
       "174797                       Oregon      0  \n",
       "174798  Kathmandu Metropolitan City      1  \n",
       "\n",
       "[174799 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "prompt_qc = \"\".join([f\"<PREFIX{i}>\" for i in range(num_prefix_tokens-1)])+\"Question: {question} Context: {context}\"\n",
    "prompt_a = \"\".join([f\"<PREFIX{i}>\" for i in range(num_prefix_tokens-1)])+\"Answer: {answers}\"\n",
    "\n",
    "loader = DataLoader(df_hf[\"train\"], batch_size = 10, collate_fn=ClassifierCollator(enc_tokenizer, prompt_qc, prompt_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from train_utils.Trainer import CustomTrainer, UpdateOutputDirCallback\n",
    "import wandb\n",
    "\n",
    "wandb.login(key = \"f190694cef6354f5205256582202a2b16502a236\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size= 32,\n",
    "                                  gradient_accumulation_steps= 32,\n",
    "                                  warmup_steps= 500,\n",
    "                                  num_train_epochs= 2,\n",
    "                                  learning_rate= 1e-4,\n",
    "                                  fp16= False,\n",
    "                                  logging_steps= 100,\n",
    "                                  evaluation_strategy= \"epoch\",\n",
    "                                  save_strategy= \"epoch\",\n",
    "                                  output_dir= \"/netscratch/roeder/classifier_train\",\n",
    "                                  optim= \"adamw_torch\",)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args= training_args,\n",
    "    train_dataset=df_hf[\"train\"],\n",
    "    eval_dataset=df_hf[\"validation\"],\n",
    "    tokenizer=enc_tokenizer,\n",
    "    data_collator=ClassifierCollator(enc_tokenizer, prompt_qc, prompt_a),\n",
    "    callbacks=[UpdateOutputDirCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceTransformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"squad_with_answer_sentence\")\n",
    "\n",
    "df_pd_train = dataset[\"train\"].to_pandas()\n",
    "df_pd_validation = dataset[\"validation\"].to_pandas()\n",
    "\n",
    "df_pd_train = prepare_dataset(df_pd_train)\n",
    "df_pd_validation = prepare_dataset(df_pd_validation)\n",
    "\n",
    "df_pd_train = df_pd_train[df_pd_train.false_answers.apply(len)>0]\n",
    "df_pd_validation = df_pd_validation[df_pd_validation.false_answers.apply(len)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "        query_text = prompt_qc.format(question=row[\"question\"], context=row[\"context\"])\n",
    "        pos_text = prompt_a.format(answers=row[\"answers\"])\n",
    "        neg_text = [prompt_a.format(answers=f_answer) for f_answer in row[\"false_answers\"]]\n",
    "\n",
    "        return InputExample(texts=[query_text, pos_text, neg_text[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Daniel/.cache\\torch\\sentence_transformers\\roberta-base. Creating a new one with MEAN pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at C:\\Users\\Daniel/.cache\\torch\\sentence_transformers\\roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_prefix_tokens  = 3\n",
    "\n",
    "model = SentenceTransformer(\"roberta-base\")\n",
    "model.tokenizer.model_max_length = 512\n",
    "\n",
    "word_embedding_model = model._first_module()\n",
    "\n",
    "tokens = [f\"<PREFIX{i}>\" for i in range(num_prefix_tokens-1)]\n",
    "word_embedding_model.tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n",
    "\n",
    "prompt_qc = \"\".join([f\"<PREFIX{i}>\" for i in range(num_prefix_tokens-1)])+\"Question: {question} Context: {context}\"\n",
    "prompt_a = \"\".join([f\"<PREFIX{i}>\" for i in range(num_prefix_tokens-1)])+\"Answer: {answers}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='C:\\Users\\Daniel/.cache\\torch\\sentence_transformers\\roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t50265: AddedToken(\"<PREFIX0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50266: AddedToken(\"<PREFIX1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CustomDataset(df_pd_train)\n",
    "loader = DataLoader(df, batch_size=2, shuffle=True)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_objectives=[(loader, train_loss)],\n",
    "          epochs=1,\n",
    "          warmup_steps=200,\n",
    "          use_amp=True,\n",
    "          checkpoint_path=\"test\",\n",
    "          checkpoint_save_steps=len(loader),\n",
    "          optimizer_params = {'lr': 1e-4},\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\Documents\\ProjectPhi\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = dataset[\"train\"].to_pandas()\n",
    "q = df_pd.iloc[0].question\n",
    "c = df_pd.iloc[0].context\n",
    "sample = tokenizer(q, c, return_tensors=\"pt\")\n",
    "\n",
    "out  = model(**sample)\n",
    "begin = out.start_logits.argmax()\n",
    "end = out.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = sample.input_ids[0, begin : end + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
